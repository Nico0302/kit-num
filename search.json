[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vorlesungsnotizen zu Numerik für Informatiker",
    "section": "",
    "text": "Einführung\nEs handelt sich hierbei um meine Vorlesungsnotizen, basierend auf den Übungsaufzeichnungen, dem offiziellen Skript von Wieners (2025), sowie Passagen aus Bartels (2016) und dem Skript von Weiß (2024).\n\n\n\n\n\n\nVorsicht\n\n\n\nDie Notizen sind nicht vollständig und dienen lediglich als Ergänzung zu den Vorlesungsunterlagen.\nSolltest du einen Fehler finden, kannst du ein Issue anlegen.\n\n\n\n\n\n\nBartels, Sören. 2016. Numerik 3x9: Drei Themengebiete in jeweils neun kurzen Kapiteln. 1. Aufl. 2016. Springer-Lehrbuch. Berlin Heidelberg: Springer Spektrum. https://doi.org/10.1007/978-3-662-48203-2.\n\n\nWeiß, Daniel. 2024. „Numerische Mathematik für die Fachrichtungen Informatik und Ingenieurwesen“. Karlsruher Institut für Technologie.\n\n\nWieners, Christian. 2025. „Einführung in die Numerische Mathematik“. Karlsruher Institut für Technologie.",
    "crumbs": [
      "Einführung"
    ]
  },
  {
    "objectID": "01-arithmetik.html",
    "href": "01-arithmetik.html",
    "title": "1  Arithmetik",
    "section": "",
    "text": "1.1 Gleitkommazahlen\nWir betrachten für eine gegebene Basis \\(B \\geq 2\\), einen minimalen Exponent \\(E^{-}\\) und Längen \\(M\\) und \\(E\\) die endliche Menge der normalisierten Gleitpunktzahlen \\(\\mathrm{FL}\\).\n\\[\n\\mathrm{FL}:=\\{ \\pm B^e \\underbrace{\\sum_{l=1}^M a_l B^{-l}}_{=m} \\; | \\; e=E^{-}+\\sum_{k=0}^{E-1} c_k B^k, \\  a_l, c_k \\in\\{0, \\ldots, B-1\\}, \\ a_1 \\neq 0\\} \\cup\\{0\\}\n\\]\nMaschienengenauigkeit\n\\[\n\\text{eps} := \\sup \\left\\{ \\frac{|x - fl(x)|}{|x|} \\ | \\ 1 &lt; x &lt; 2 \\right\\} = \\frac{B^{1-M}}{2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#auslöschung",
    "href": "01-arithmetik.html#auslöschung",
    "title": "1  Arithmetik",
    "section": "1.2 Auslöschung",
    "text": "1.2 Auslöschung\n\nN = 2**10\n\ndef exp(x):\n    \"\"\"\n    Compute the exponential function using Taylor series expansion.\n    \"\"\"\n    return np.sum([x**n / math.factorial(n) for n in range(N)], axis=0)\n\nx = 10\n\nz_bad = exp(-x)\nz_good = 1 / exp(x)\n\nr = np.exp(-x) # reference\n\nnp.abs(z_bad - r) / r, np.abs(z_good - r) / r\n\n(np.float64(6.529424994681785e-09), np.float64(1.4925713791816933e-16))\n\n\nQuadratische Gleichung\nAnstatt \\(x_2=p-\\sqrt{p^2-q}\\), verwenden wir\n\\[\nx_2=p-\\sqrt{p^2-q} \\cdot \\frac{p+\\sqrt{p^2+q}}{p+\\sqrt{p^2+q}} = \\frac{q}{p+\\sqrt{p^2-q}}=\\frac{q}{x_1}\n\\]\n(Satz von Vieta) um die Auslöschung zwischen \\(p\\) und \\(\\sqrt{p^2-q}\\) zu vermeiden.\n\np = 1e10\nq = 1e2\n\nprint(np.roots([1, -2*p, q])) # reference\n\nx1 = p + math.sqrt(p**2 - q)\n\nx2_bad = p - math.sqrt(p**2 - q)\nx2_good = q / x1\n\nx2_bad, x2_good\n\n[2.e+10 5.e-09]\n\n\n(0.0, 5e-09)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#kondition-und-stabilität",
    "href": "01-arithmetik.html#kondition-und-stabilität",
    "title": "1  Arithmetik",
    "section": "1.3 Kondition und Stabilität",
    "text": "1.3 Kondition und Stabilität\n\nDie Kondition eines Problems ist ein Maß dafür, wie stark die Abhängigkeit der Lösung von den Daten ist.\nAbsolute Konditionszahl\n\\[\n\\kappa_\\text{abs}(x) = | f'(x) |\n\\]\nRelative Konditionszahl\n\\[\n\\kappa_\\text{rel}(x) = \\frac{| f'(x) |}{|f(x)|} \\cdot |x|\n\\]\nMatrix Kondition\n\\[\n\\kappa_p(A) = ||A||_p \\cdot ||A^{-1}||_p \\quad \\text{für } p = 1,2,\\infty\n\\]\n\n\n\n\n\n\nHinweis\n\n\n\nFür symmetrische Matrizen (\\(A=A^\\top\\)) gilt:\n\n\\(\\sigma(A) \\subset \\mathbb{R}\\) (Spektrum bzw. alle Eigenwerte sind reell)\n\\(\\|A\\|_2=\\rho(A)\\) (Septralradius bzw. größter Eigenwert im Betrag)\n\\(\\kappa_2(A)=\\frac{\\max _{\\lambda \\in \\sigma(A)}|\\lambda|}{\\min _{\\lambda \\in \\sigma(A)}|\\lambda|}\\) (Verhältnis der größten zur kleinsten Eigenwerte im Betrag)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#vektor--und-matrixnormen",
    "href": "01-arithmetik.html#vektor--und-matrixnormen",
    "title": "1  Arithmetik",
    "section": "1.4 Vektor- und Matrixnormen",
    "text": "1.4 Vektor- und Matrixnormen\nEine Norm auf \\(\\mathbb{R}^n\\) ist eine Abbildung \\(\\|\\cdot\\|: \\mathbb{R}^n \\rightarrow \\mathbb{R}_{\\geq 0}\\) mit den folgenden Eigenschaften:\n\n\\(\\|x\\|=0 \\Longrightarrow x=0\\) für alle \\(x \\in \\mathbb{R}^n\\) (Definitheit);\n\\(\\|x+y\\| \\leq\\|x\\|+\\|y\\|\\) für alle \\(x, y \\in \\mathbb{R}^n\\) (Dreiecksungleichung);\n\\(\\|\\lambda x\\|=|\\lambda|\\|x\\|\\) für alle \\(\\lambda \\in \\mathbb{R}\\) und \\(x \\in \\mathbb{R}^n\\) (Homogenität).\n\nWir verwenden für \\(x \\in \\mathbb{R}^N\\) und \\(A \\in \\mathbb{R}^{M \\times N}\\)\n\\[\n\\begin{array}{ll}\n|x|_1=\\sum_{n=1}^N\\left|x_n\\right| & \\text { 1-Norm } \\\\\n|x|_2=\\sqrt{x^T x}=\\left(\\sum_{n=1}^N\\left|x_n\\right|^2\\right)^{\\frac{1}{2}} & \\text { Euklidische Norm } \\\\\n|x|_{\\infty}=\\max _{n=1, \\ldots, N}\\left|x_n\\right| & \\text { Supremumsnorm }\n\\end{array}\n\\]\nFür Matrizen \\(A \\in \\mathbb{R}^{M \\times N}\\) definieren wir eine allgemeine Norm mit\n\\[\\|A\\|_{p}=\\sup _{x \\in \\mathbb{R}^n,\\|x\\|=1}\\|A x\\|=\\inf \\left\\{c \\geq 0: \\forall x \\in \\mathbb{R}^n\\|A x\\| \\leq c\\|x\\|\\right\\}\\]\nund spezifizieren sie für \\(p=1,2,\\infty,F\\) wie folgt\n\\[\n\\begin{array}{ll}\n\\|A\\|_1=\\max _{n=1, \\ldots, N} \\sum_{m=1}^M|A[m, n]| & \\text { Spaltensummennorm, } \\\\\n\\|A\\|_2=\\sqrt{\\rho\\left(A^T A\\right)} & \\text { Spektralnorm, } \\\\\n\\|A\\|_{\\infty}=\\max _{m=1, \\ldots, M} \\sum_{n=1}^N|A[m, n]| & \\text { Zeilensummennorm, } \\\\\n\\|A\\|_F=\\left(\\sum_{m=1}^M \\sum_{n=1}^N A[m, n]^2\\right)^{\\frac{1}{2}} & \\text { Frobeniusnorm. }\n\\end{array}\n\\]\nDabei ist\n\\[\n\\begin{aligned}\n& \\rho(A)=\\max \\{|\\lambda|: \\lambda \\in \\sigma(A)\\}  \\text { Spektralradius, }  \\\\\n& \\sigma(A)=\\left\\{\\lambda \\in \\mathbb{C}: \\operatorname{det}\\left(A-\\lambda I_N\\right)=0\\right\\}  \\text { Spektrum. }\n\\end{aligned}\n\\]\nEs gilt immer\n\\[|A x|_p \\leq\\|A\\|_p|x|_p\\]\nfür alle \\(x \\in \\mathbb{R}^N\\) und wegen \\(\\|A\\|_2 \\leq\\|A\\|_F\\) auch\n\\[\n|A x|_2 \\leq\\|A\\|_2|x|_2 \\leq\\|A\\|_F|x|_2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "02-lgs.html",
    "href": "02-lgs.html",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "",
    "text": "2.1 Vorwärts-Substitution\n\\[\nL = \\begin{pmatrix}\n1      \\\\\nl_{21} & 1      \\\\\nl_{31} & l_{32} & 1      \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\nl_{n1} & l_{n2} & l_{n3} & \\dots  & 1 \\\\\n\\end{pmatrix}\n\\]\nDie Vorwärts-Substitution löst \\(L\\cdot\\mathbf{y}=\\mathbf{b}\\) (normierte untere Dreiecksmatrix), indem wir über die Zeilen iterieren und dabei die Lösungen der vorheringen \\(\\mathbf{x}_j\\) für die Berechung des aktuellen \\(\\mathbf{x}_i\\) verwenden (\\(\\mathbf{x}_1 = \\mathbf{b}_1\\)).\nDie Laufzeit liegt somit in \\(O(n^2)\\).\ndef forward_sub(lower, rhs):\n    n = lower.shape[0]\n    solution = np.zeros(n)\n    for i in range(n):\n        solution[i] = rhs[i]\n        for j in range(i):\n            solution[i] -= lower[i, j] * solution[j]\n            solution[i] = solution[i] / lower[i, i]\n    return solution\nforward_sub(np.array([\n    [1, 0, 0], \n    [2, 1, 0], \n    [3, 4, 1]]\n), np.array([1, 2, 3]))\n\narray([1., 0., 0.])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#rückwärts-substitution",
    "href": "02-lgs.html#rückwärts-substitution",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.2 Rückwärts-Substitution",
    "text": "2.2 Rückwärts-Substitution\n\\[\nR = \\begin{pmatrix}\nr_{11} & r_{12} & r_{13} & \\dots  & r_{1n} \\\\\n    & r_{22} & r_{23} & \\dots  & r_{2n} \\\\\n    &        & r_{33} & \\dots  & r_{3n} \\\\\n    &        &        & \\ddots & \\vdots \\\\\n    &        &        &        & r_{nn}\n\\end{pmatrix}\n\\]\nDie Rückwärts-Substitution löst \\(R \\cdot \\mathbf{x}=\\mathbf{y}\\), indem wir von der letzten Zeile aus das verfahren der Vorwärts-Substitution anwenden.\nDie Laufzeit liegt somit ebenfalls in \\(O(n^2)\\).\n\ndef backward_sub(upper, rhs):\n    n = upper.shape[0]\n    solution = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        tmp = rhs[i]\n        for j in range(i + 1, n):\n            tmp -= upper[i, j] * solution[j]\n            solution[i] = tmp / upper[i, i]\n    return solution\n\n\nbackward_sub(np.array([\n    [2, 2, 3], \n    [0, 1, 4], \n    [0, 0, 1]]\n), np.array([1, 0, 0]))\n\narray([0.5, 0. , 0. ])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#lr-zerlegung",
    "href": "02-lgs.html#lr-zerlegung",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.3 LR-Zerlegung",
    "text": "2.3 LR-Zerlegung\n(en. LU-Decomposition)\n\n\n\n\n\n\n\nWarnung\n\n\n\nDie \\(1\\)-en auf der Diagonalen der \\(L\\)-Matrix bleiben beim Zeilentauschen unverändert\n\n\nDie \\(LR\\)-Zerlegung lässt sich mittels des Gauß-Algorithmus bestimmen, indem wir \\(A\\) auf eine untere Dreiecksmatrix \\(R\\) gaußen und uns die Operationen in \\(L\\) “merken”. Sie ist eindeutig und benötigt \\(O(n^3)\\) Operationen.\nDie Berechnung ist nicht stabil.\nHinreichende Bedingungen für die Exsistenz einer \\(LR\\)-Zerlegung für eine quadratische Matrix \\(A\\):\n\nstrikt diagonal-dominant, daher das Diagonalelement ist größer als die Summe aller anderen Elemente in der Zeile, bzw.\n\n\\[\n|A[n, n]|&gt;\\sum_{\\substack{k=1 \\\\ k \\neq n}}^N|A[n, k]| \\quad \\text { für } n=1, \\ldots, N\n\\]\n\npositiv definit, daher alle Eigenwerte \\(&gt; 0\\), bzw.\n\n\\[\nx^{\\top} A x&gt;0 \\quad \\text { für alle } x \\in \\mathbb{R}^N, x \\neq 0 .\n\\]\n\n\n\n\n\n\nHinweis\n\n\n\n\n\n\nHauptminorenkriterium für positiv definite Matrizen\n\n\n\n\nFalls diese Bedingungen nicht gegeben sind, können wir mittels Zeilenvertauschung (Permutationsmatrix \\(P\\)) eine LR-zerlegbare Matrix \\(PA\\) in \\(O(n^3)\\) erzeugen.\n\ndef lu_decomposition(matrix):\n    n = matrix.shape[0]\n    lower = np.zeros(shape=matrix.shape)\n    upper = np.zeros(shape=matrix.shape)\n    for j in range(n):\n        lower[j][j] = 1.0\n        for i in range(j + 1):\n            first_sum = sum(upper[k][j] * lower[i][k] for k in range(i))\n            upper[i][j] = matrix[i][j] - first_sum\n        for i in range(j, n):\n            second_sum = sum(upper[k][j] * lower[i][k] for k in range(j))\n            lower[i][j] = (matrix[i][j] - second_sum) / upper[j][j]\n    return lower, upper\n\ndef solve_with_lu(matrix, rhs):\n    lower, upper = lu_decomposition(matrix)\n    y = forward_sub(lower, rhs)\n    return backward_sub(upper, y)\n\n\nmatrix = np.array([[2.0, 1.0],\n[1.0, 4.0]])\nrhs = np.array([1.0, 2.0])\nsolution = solve_with_lu(matrix, rhs)\nprint(\"solution\", solution)\ntest = rhs - np.dot(matrix, solution)\nprint(\"test \",test)\n\nsolution [0.5 0. ]\ntest  [0.  1.5]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#choelsky-zerlegung",
    "href": "02-lgs.html#choelsky-zerlegung",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.4 Choelsky-Zerlegung",
    "text": "2.4 Choelsky-Zerlegung\n\n\n\n\n\n\n(2.7) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) symmetrisch und positiv definit. Dann existiert genau eine Cholesky-Zerlegung \\(A=L L^{\\top}\\) mit einer regulären unteren Dreiecksmatrix \\(L\\).\n\n\nEs handelt sich somit um eine Spezialisierung der LR-Zerlegung für symmetrisch, positiv definite Matrizen.\n\\(\\renewcommand{\\b}[1]{\\color{teal}{#1}\\color{black}}\\renewcommand{\\o}[1]{\\color{orange}{#1}\\color{black}}\\)\n\\[\\begin{align}\nA &=\n\\begin{pmatrix}\n   a_{11} & a_{21} & a_{31}\\\\\n   a_{21} & a_{22} & a_{32}\\\\\n   a_{31} & a_{32} & a_{33}\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n   l_{11} & 0 & 0 \\\\\n   l_{21} & l_{22} & 0 \\\\\n   l_{31} & l_{32} & l_{33}\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n   l_{11} & l_{21} & l_{31} \\\\\n   0 & l_{22} & l_{32} \\\\\n   0 & 0 & l_{33}\n\\end{pmatrix} \\equiv L L^T \\\\\n&= \\begin{pmatrix}\n   l_{11}^2     & l_{21}l_{11} & l_{31}l_{11} \\\\\n   l_{21}l_{11} & l_{21}^2 + l_{22}^2& l_{31}l_{21}+l_{32}l_{22} \\\\\n   l_{31}l_{11} & l_{31}l_{21}+l_{32}l_{22} & l_{31}^2 + l_{32}^2+l_{33}^2\n\\end{pmatrix}\\end{align}\\]\n\n2.4.1 Berechnung\nDiagonalelemente:\n\\[l_{kk} = \\sqrt{a_{kk} - \\sum_{\\b{j}=1}^{k-1} l_{k\\b{j}}^2}\\]\nRest:\n\\[l_{\\o{i}k} = \\frac{1}{l_{kk}} \\left ( a_{ik} - \\sum_{\\b{j}=1}^{k-1} l_{\\o{i}\\b{j}}l_{k\\b{j}} \\right )\\]\n\ndef cholesky_decomposition(A):\n    n = matrix.shape[0]\n    lower = np.zeros(matrix.shape)\n    lower[0, 0] = np.sqrt(matrix[0, 0])\n    for n in range(1, n):\n        y = forward_sub(lower[:n, :n], matrix[n, :n]) # linalg.solve_triangular(lower[:n, :n], matrix[n, :n], lower=True)\n        lower[n, :n] = y\n        lower[n, n] = np.sqrt(matrix[n, n] - np.dot(y, y))\n    return lower\n\ndef solve_with_cholesky(matrix, rhs):\n    lower = cholesky_decomposition(matrix)\n    y = forward_sub(lower, rhs)\n    return backward_sub(lower.transpose(), y)\n\n\nmatrix = np.array([[2.0, 1.0],\n[1.0, 4.0]])\nrhs = np.array([1.0, 2.0])\nrhs = np.array([1.0, 2.0])\nsolution = solve_with_cholesky(matrix, rhs)\nprint(\"solution\",solution)\ntest = rhs - np.dot(matrix, solution)\nprint(\"test \",test)\n\nsolution [0.70710678 0.        ]\ntest  [-0.41421356  1.29289322]\n\n\n\nDie Cholesky-Zerlegung ist stabil: Es gilt \\(\\kappa_2(L)^2=\\kappa(A)\\)\nDie Berechnung der Cholesky-Zerlegung benötigt nur halbsoviele Operationen wie die Berechnung einer LR-Zerlegung.\nMatrizen mit einer geeigneten Hüllenstruktur (viele Nullelemente wie bei der Bandmatrix) können effizienter gelöst werden (Bandmatrix in \\(O(NM^2)\\))\n\n\n\n\nSchematische Darstellung einer Bandmatrix",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#qr-zerlegung",
    "href": "02-lgs.html#qr-zerlegung",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.5 QR-Zerlegung",
    "text": "2.5 QR-Zerlegung\n\n\n\n\n\n\n(2.14) Satz (QR-Zerlegung)\n\n\n\nZu \\(A \\in \\mathbb{R}^{M \\times N}\\) existiert eine \\(Q R\\)-Zerlegung \\(A=Q R\\) in eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{M \\times M}\\) mit \\(Q^{\\top} Q=I_M\\) und eine obere Dreiecksmatrix \\(R \\in \\mathbb{R}^{M \\times N}\\) mit \\(R[m, n]=0\\) für \\(m&gt;n\\).\n\n\n\nDas LGS \\(Ax = b\\) kann durch die Berechnung \\(y = Q^\\top b\\) und darauf mit Rücksubstitution \\(Rx = y\\) gelöst werden.\nAsymptotischer Aufwand in \\(O(N^3)\\)\n\nRotationen und Drehungen sind orthogonale Matrizen \\(Q \\in \\mathbb{R}^{N \\times N}\\) mit\n\n\\(Q Q^{\\top}=I_N, \\ Q^{\\top} Q=I_N\\), so dass \\(Q^{-1}=Q^{\\top}\\),\n\\(|Q v|_2=|v|_2\\) und \\((Q v)^{\\top}(Q w)=v^{\\top} w\\) Längen und Winkel erhaltend,\n\\(\\kappa_2(Q)=1\\).\n\n\n2.5.1 Householder Transformationen\nWir erhalten \\(N\\) orthogonale Vektoren für \\(A\\), indem wir den ersten Spaltenvektor \\(v_1\\) mittels einer Householder Transformation (Spiegelung) \\(Q_1\\) auf die \\(x\\)-Achse (\\(e_1\\)) abbilden und dies sukzessiv für die nächsten Spaltenvektoren \\(v_i\\) aus \\(Q_1 \\cdot \\dots \\cdot Q_{i-1} \\cdot A\\) wiederholen (dabei vernachlässigen wir die ersten \\(i\\) Zeilen, da wir nur einen Untervektorraum in \\(\\mathbb{R}^{N-i}\\) betrachten).\n\n\n\nQR-Zerlegung mittels Householder Transformationen berechnen\n\n\n\n\n2.5.2 Givens-Rotation\nAlternativ können wir Givens-Rotationen verwenden, um die Matrix \\(A\\) in eine obere Dreiecksmatrix zu überführen. Eine Givens-Rotation ist eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{N \\times N}\\), die die \\(m\\)-te und \\(n\\)-te Zeile von \\(A\\) rotiert, um die Elemente unterhalb der Hauptdiagonalen zu eliminieren.\n\n\n\nQR-Zerlegung mittels Givens-Rotation\n\n\nBesonders bei dünnbesetzten Matrizen (sparse) ist die Givens-Rotation effizient.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "03-fitting.html",
    "href": "03-fitting.html",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "",
    "text": "3.1 Normalengleichung\nAnstatt eine extakte Lösung zu finden, können wir auch eine Näherungslösung suchen, die die Summe der Abweichungen minimiert. Dies wird als lineare Ausgleichsrechnung bezeichnet.\n\\[\nA^\\top Ax = A^\\top b\n\\]\nist äquivalent zur Minimierung von \\(\\|Ax - b\\|_2\\)\nLösungsverfahren:\nFalls \\(A^\\top A\\) invertierbar und gut konditioniert:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#normalengleichung",
    "href": "03-fitting.html#normalengleichung",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "",
    "text": "Wenn \\(A^\\top A\\) invertierbar ist, ist die Lösung eindeutig.\nWenn \\(\\operatorname{Kern} \\{v \\in \\mathbb{R}^N : Av=0\\} \\neq \\{0\\}\\), dann ist die Lösung nicht eindeutig. In diesem Fall ist das Problem nicht sachgemäß gestellt!\n\n\n\n\nBerechne \\(QR\\)-Zerlegung von \\(A=QR\\)\n\n\\(R[1:N, 1:N]x = (Q^T b)[1:N]\\) lösen\n\nBerechne \\(A^\\top A\\) mittels Cholesky-Zerlegung",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#singularitätszerlegung",
    "href": "03-fitting.html#singularitätszerlegung",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "3.2 Singularitätszerlegung",
    "text": "3.2 Singularitätszerlegung\n(en. Singular Value Decomposition, SVD)\n\nFalls \\(A\\) singulär (nicht invertierbar) oder schlecht konditioniert ist, kann die Singularitätszerlegung verwendet werden um die Normalengleichung zu lösen.\n\n\n\n\n\n\n(3.4) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{K \\times N}\\) mit \\(R = \\text{rang} A\\). Dann existieren Singulärwerte \\(\\sigma_1, \\dots, \\sigma_R &gt; 0\\), Matrizen \\(V \\in \\mathbb{R}^{K \\times R}, U \\in \\mathbb{R}^{N \\times R}\\) mit \\(V^\\top V = I_R = U^\\top U\\) und eine Zerlegung\n\\[\nA = U \\Sigma V^\\top\n\\]\nmit \\(\\Sigma = \\text{diag}(\\sigma_1, \\dots, \\sigma_R)= \\text{diag}(\\sqrt{\\lambda_1}, \\dots, \\sqrt{\\lambda_R}) \\in \\mathbb{R}^{R \\times R}\\).\n\n\nWir führen eine Drehung mit \\(V^\\top\\) durch, skalieren mit den Singularitätswerten und passen die Dimension von \\(N\\) auf \\(K\\) mit \\(\\Sigma\\) und drehen dann in \\(\\mathbb{R}^K\\) mittels \\(U\\).\n\n\\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_R &gt; 0\\) sind die geordneten Eigenwerte von \\(A^\\top A\\) und \\(A A^\\top\\).\n\\(U\\) enthält die Eigenvektoren von \\(A A^\\top\\) als Spaltenvektoren und \\(V\\) die Eigenvektoren von \\(A^\\top A\\) als Zeilenvektoren (transponiert).\nEs gilt \\(A = \\sum_{k=1}^R \\sigma_k v_k u_k^\\top\\).\nDie Spalten \\(v_1,...,v_R\\) von V bilden eine ONB von \\(\\operatorname{Bild} A\\).\nDie Spalten \\(u_1,...,u_R\\) von U lassen sich mit \\(u_{R+1},...,u_N\\) zu einer ONB von \\(\\mathbb{R}^N\\) ergänzen. Dann ist \\(u_{R+1},...,u_N\\) eine ONB von \\(\\operatorname{Kern} A\\).\n\\(A^+ := \\sum_{k=1}^R \\frac{1}{\\sigma_k} u_k v_k^T = U \\Sigma^{-1} V^T\\) heißt Pseudo-Inverse.\nIst \\(A\\) regulär, so gilt \\(A^{-1} = A^+\\). Die Normalengleichung und damit \\(\\|Ax-b\\|_2 = \\min!\\) wird durch \\(x = A^+b\\) gelöst.\n\\(\\|A\\|_2 = \\sigma_1\\) und \\(\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_R}\\). Berechnung aufwendig, erfordert Eigenwertberechnung von \\(S = A^T A\\). Stabile Berechnung mit \\(O(N^3)\\) Operationen!\nDie Berechnung von \\(A^+b\\) ist für \\(\\sigma_R \\ll 1\\) numerisch nicht stabil.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#tikhonov-regularisierung",
    "href": "03-fitting.html#tikhonov-regularisierung",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "3.3 Tikhonov-Regularisierung",
    "text": "3.3 Tikhonov-Regularisierung\nStabilisiert die Singularitätszerlegung, indem ein Regularisierungsterm hinzugefügt wird.\n\n\n\n\n\n\n(3.5) Satz\n\n\n\nZu \\(A \\in \\mathbb{R}^{M \\times N}, b \\in \\mathbb{R}^M, \\alpha &gt; 0\\) existiert genau ein \\(x^\\alpha \\in \\mathbb{R}^N\\), das die Tikhonov-Regularisierung \\[\nF_\\alpha(x) := \\frac{1}{2}|Ax-b|_2^2 + \\frac{\\alpha}{2}|x|_2^2\n\\] minimiert. Es gilt \\[\nx^\\alpha = (A^\\top A + \\alpha I_N)^{-1} A^\\top b.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html",
    "href": "04-eigenwerte.html",
    "title": "4  Eigenwertberechnung",
    "section": "",
    "text": "4.1 Hessenberg-Matrix\n🏁 Ziel: Suche iterativ eine zu \\(A=A_0\\) ähnliche (gleiche Eigenwerte) Matrix \\(A_k\\), die wir mittels der QR-Zerlegung in eine obere Dreiecksmatrix \\(R\\) überführen können:\n\\[\nA_k \\to R = \\begin{pmatrix} \\lambda_1 & * & \\cdots & * \\\\ & \\lambda_2 & \\ddots & \\vdots \\\\ & & \\ddots & * \\\\ & & & \\lambda_N \\end{pmatrix},\nQ_k \\to I_N\n\\]\nSomit lassen sich die Eigenwerte von \\(A_k\\) (\\(k \\to \\infty\\)) auf der Diagonalen ablesen (bzw. approximieren).\n⚠️ Problem: Normale QR-Zerlegung mit \\(A_{k+1} = R_k Q_k\\) benötigt \\(O(N^3)\\) Operationen.\n\\[\nH = \\begin{pmatrix}\n* & * & * & * & * \\\\\n* & * & * & * & * \\\\\n0 & * & * & * & * \\\\\n0 & 0 & * & * & * \\\\\n0 & 0 & 0 & * & *\n\\end{pmatrix}\n\\]\nEine Matrix \\(H \\in \\mathbb{R}^{N \\times N}\\) heißt (obere) Hessenberg-Matrix, wenn \\(H[n+2:N, n] = 0_{N-n-1}\\) für \\(n = 1,...,N-2\\) gilt.\nA = np.array([\n    [8, 12/5, -9/5],\n    [12/5, 109/25, 12/25],\n    [-9/5, 12/25, 116/25]\n])\n\nprint(H := scipy.linalg.hessenberg(A))\n\nprint(np.linalg.eig(A).eigenvalues)\nprint(np.linalg.eig(H).eigenvalues)\n\n[[8 -3 0]\n [-3 4 0]\n [0 0 5]]\n[2244241/233640 559439/233640 5]\n[2244241/233640 559439/233640 5]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html#hessenberg-matrix",
    "href": "04-eigenwerte.html#hessenberg-matrix",
    "title": "4  Eigenwertberechnung",
    "section": "",
    "text": "(4.3) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\). Dann existiert eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{N \\times N}\\), so dass \\[\nH = QAQ^{\\top}\n\\] eine Hessenberg-Matrix ist.\nDie Berechnung von \\(Q\\) benötigt \\(O(N^3)\\) Operationen.\n\n\n\n\n\nBerechnung der Hessenberg-Matrix",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html#inverse-iteration-mit-shift",
    "href": "04-eigenwerte.html#inverse-iteration-mit-shift",
    "title": "4  Eigenwertberechnung",
    "section": "4.2 Inverse Iteration mit Shift",
    "text": "4.2 Inverse Iteration mit Shift\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) symmetrisch.\n\nWähle \\(v^0 \\in \\mathbb{R}^N\\) mit \\(|v^0| = 1\\).  Setze \\(k := 0\\) und wähle \\(\\varepsilon &gt; 0\\).\nBerechne \\[\n\\mu_k = r(A, v^k) = \\frac{v^{\\top} A v}{v^{\\top} v},\n\\].  Falls \\(|Av^k - \\mu_k v^k| &lt; \\varepsilon\\): STOP.\nBerechne \\[\\begin{align*}\nw^k &= (A - \\mu_k I_N)^{-1}v^k, \\\\\nv^{k+1} &= \\frac{1}{|w^k|}w^k.\n\\end{align*}\\]\nSetze \\(k := k + 1\\) und gehe zu 1..\n\n\\(r(A, w) \\approx \\lambda\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html#qr-iteration-mit-shift",
    "href": "04-eigenwerte.html#qr-iteration-mit-shift",
    "title": "4  Eigenwertberechnung",
    "section": "4.3 QR-Iteration mit Shift",
    "text": "4.3 QR-Iteration mit Shift\n💡 Idee: Transformiere \\(A\\) auf ähnliche Hessenbergform und führe die QR-Zerlegung mit Shift in \\(O(N^2)\\) durch.\nIm Folgenden sei \\(A \\in \\mathbb{R}^{N \\times N}\\) symmetrisch, tridiagonal und irreduzibel (\\(A\\) kann z.B. eine Hessenberg-Matrix sein).\n\nWähle \\(\\varepsilon \\geq 0\\), setze \\(A_0 = A\\), \\(k := 0\\) und \\(n=N\\).\nFalls \\(|A_k[n, n-1]| \\leq \\varepsilon\\)  setze \\(n := n - 1\\),  falls \\(n = 1\\): STOP\nWähle \\(\\mu_k = A[n, n]\\).\nBerechne eine QR-Zerlegung \\(A_k - \\mu_k I_N = Q_k R_k\\).\nSetze  \\(A_{k+1} = R_k Q_k + \\mu_k I_N\\).  Setze \\(k := k + 1\\), gehe zu 1.\n\nEs gilt \\[\\begin{alignat*}{2}\nA_{k+1} &= R_k Q_k &&+ \\mu_k I_N \\\\\n&= Q_k^\\top (A_k - \\mu_k I_N) Q_k &&+ \\mu_k I_N \\\\\n&= Q_k^\\top A_k Q_k\n\\end{alignat*}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html",
    "href": "05-iter-lgs.html",
    "title": "5  Iterationsverfahren für lineare Gleichungssysteme",
    "section": "",
    "text": "5.1 Allgemeine lineare Iteration\n🏁 Ziel: Approximierung eines LGS \\(Ax = b\\) mittels einer iterativen Methode und einer Genauigkeit \\(\\varepsilon\\)\n⚠️ Problem: \\(A\\) ggf. schlecht konditioniert\n💡 Idee: Löse \\(BAx=Bb\\) mit \\(B\\) invertierbar und \\(BA\\) kleinere Kondition als \\(A\\)\nSei \\(B \\in \\mathbb{R}^{N \\times N}\\) ein Vorkonditionierer.\nSomit erhalten wir für \\(x\\) eine Fixpunktaufgabe:\n\\[\nx = (I - BA)x + Bb = x + B(b - Ax)\n\\]\nZur Bestimmung von \\(B\\) zerlegen wir\n\\[\nA = L + D + R\n\\]\nmit der strikt unteren Dreiecksmatrix \\(L=\\text{lower}(A)\\), der Diagonalmatrix \\(D=\\text{diag}(A)\\) und der strikt oberen Dreiecksmatrix \\(R=\\text{upper}(A)\\). Damit erhalten wir die folgenden Verfahren:\nAlgorithmus\nBemerkungen",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterationsverfahren für lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#allgemeine-lineare-iteration",
    "href": "05-iter-lgs.html#allgemeine-lineare-iteration",
    "title": "5  Iterationsverfahren für lineare Gleichungssysteme",
    "section": "",
    "text": "\\(B = D^{-1}\\) Jacobi,\n\\(B = (L+D)^{-1}\\) Gauß-Seidel,\n\\(B = R^{-1}L^{-1}\\) mit \\(A \\approx LR\\) unvollständige LR-Zerlegung,\n\\(B = \\sum_{j=1}^J E_j A_j^{-1} E_j^{\\top}\\) (“Subspace-Correction”) mit:  Sei \\(\\{1, \\dots, N\\} = I_1 \\cup \\dots \\cup I_J\\) eine (überlappende) Zerlegung mit \\(I_j = \\{n_{j,1}, \\dots, n_{j,N_j}\\}\\). Definiere \\(A_j[i,k] = A[n_{j,i}, n_{j,k}]\\), d.h. \\(A_j \\in \\mathbb{R}^{N_j \\times N_j}\\) und \\(E_j \\in \\mathbb{R}^{N_j \\times N}\\) mit \\(E_j[i, n_{j,i}] = 1\\) ansonsten 0.\nBPX, FETI, FETI-DP, überlappendes Schwarz-Verfahren, Mehrgitter-Verfahren, ….\n\n\n\nWähle \\(x^0 \\in \\mathbb{R}^N\\) und \\(\\varepsilon &gt; 0\\).  Setze \\(k := 0\\) und \\(r^0 := b - Ax^0\\).\nFalls \\(|r^k| &lt; \\varepsilon\\): STOP.\nBerechne:  \\[\\begin{align*}\nc^k  &= Br^k, \\\\\nx^{k+1} &= x^k + c^k, \\\\\nr^{k+1} &= r^k - Ac^k.\n\\end{align*}\\]  Setze \\(k := k + 1\\) und gehe zu 1..\n\n\n\nEs gilt  \\[\\begin{align*}\nr^{k+1} &= (b-Ax^k) - Ac^k \\\\\n&= b-A(x^k+c^k) \\\\\n&= b-Ax^{k+1}\n\\end{align*}\\]\nSei \\(Ax = b\\). Dann ist  \\(|r^k| = |b-Ax^k| = |Ax-Ax^k| \\leq \\|A\\|\\|x-x^k\\|\\).  Also folgt aus S1):  \\(|r^k| &lt; \\varepsilon\\), so dass \\(|x-x^k| = |A^{-1}r^k| &lt; \\|A^{-1}\\| \\|r^k\\| &lt; \\|A^{-1}\\| \\varepsilon\\).  Daher muss \\(\\varepsilon\\) immer problemabhängig gewählt werden!\nFür den Fehler \\(e^k = x-x^k\\) gilt: \\[\\begin{align*}\ne^{k+1} &= x-x^{k+1} \\\\\n&= x-x^k - Br^k \\\\\n&= x-x^k - B(b-Ax^k) \\\\\n&= x-x^k - B(Ax-Ax^k) \\\\\n&= (I_N - BA)(x-x^k).\n\\end{align*}\\] Hierbei handelt es sich um lineare Konvergenz, falls \\(\\|I_N - BA\\| &lt; 1\\), da \\(|x-x^k| \\leq \\|I_N - BA\\|^k \\|x-x^0\\|\\).\n\n\n\n\n\n\n\n(5.1) Satz\n\n\n\nSeien \\(A, B \\in \\mathbb{R}^{N \\times N}\\) mit \\(\\rho(I - BA) &lt; 1\\). Dann ist \\(A\\) invertierbar und für alle \\(x^0 \\in \\mathbb{R}^N\\) konvergiert die Iteration \\[\nx^{k+1} = x^k + B(b - Ax^k) \\quad k = 0, 1, 2, \\dots\n\\] linear, so dass gilt \\[\n\\lim_{k \\to \\infty} x^k = A^{-1}b.\n\\]\nDenn: Dann existiert eine Norm mit \\(||I_N - BA|| &lt; 1\\) und \\[\\begin{align*}\nA^{-1} &=  (BA)^{-1}B \\\\\n&= (I_N - I_N + BA)^{-1}B \\\\\n&= \\sum_{k=0}^{\\infty} (I_N - BA)^k B \\quad \\text{(Neumann-Reihe)}.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterationsverfahren für lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#cr-speicherformat",
    "href": "05-iter-lgs.html#cr-speicherformat",
    "title": "5  Iterationsverfahren für lineare Gleichungssysteme",
    "section": "5.2 CR-Speicherformat",
    "text": "5.2 CR-Speicherformat\n(en. Compressed Row Storage CRS)\nIm Folgenden schauen wir uns Verfahren für dünn besetzte Matrizen an. Hierfür definieren wir folgendes Speicherformat:\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) dünn besetzt. Speichere alle von null verschiedenen Einträge von \\(A\\) in einem Vektor \\(a \\in \\mathbb{R}^M\\). Speichere die zugehörigen Spaltenindizes in einem Vektor \\(s \\in \\mathbb{N}^M\\). Nun müssen wir wissen, wo die nächsten Zeilen in \\(a\\) anfangen. Speichere dafür für jede Zeile \\(i\\) in einem Vektor \\(d \\in \\mathbb{N}^{O}\\), sodass \\(d_i = \\text{Anfangsindex der Zeile } i \\text{ in } a\\) und dementsprechend \\(d_{i+1} = \\text{Beginn der nächsten Zeile } i+1\\) in \\(a\\).\n\n\n\n\n\n\nBeispiel\n\n\n\n\n\n\nCRS Speicherformat",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterationsverfahren für lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#krylovraumverfahren",
    "href": "05-iter-lgs.html#krylovraumverfahren",
    "title": "5  Iterationsverfahren für lineare Gleichungssysteme",
    "section": "5.3 Krylovraum–Verfahren",
    "text": "5.3 Krylovraum–Verfahren",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterationsverfahren für lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#cg-verfahren",
    "href": "05-iter-lgs.html#cg-verfahren",
    "title": "5  Iterationsverfahren für lineare Gleichungssysteme",
    "section": "5.4 CG-Verfahren",
    "text": "5.4 CG-Verfahren\n\n\n\nCG Algorithmus",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterationsverfahren für lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "06-newton.html",
    "href": "06-newton.html",
    "title": "6  Iterationsverfahren für nichtlineare Gleichungen",
    "section": "",
    "text": "6.1 Newton-Verfahren",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterationsverfahren für nichtlineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "06-newton.html#newton-verfahren",
    "href": "06-newton.html#newton-verfahren",
    "title": "6  Iterationsverfahren für nichtlineare Gleichungen",
    "section": "",
    "text": "Wähle den Startwert \\(x^0 \\in D\\) und die Fehlertoleranz \\(\\varepsilon\\). Setze \\(k := 0\\).\nFalls \\(|F(x^k)| &lt; \\varepsilon\\): STOP.\nBerechne die Newton-Korrektur \\(d^k\\): Löse \\[\nF'(x^k) \\cdot d^k = -F(x^k)\n\\]\nSetze  \\[\nx^{k+1} = x^k + d^k\n\\]  und \\(k := k + 1\\). Gehe dann zu S1).\n\n\n\n\n\n\n\n(6.2) Satz\n\n\n\nSei \\(D \\subset \\mathbb{R}^N\\) offen, \\(F \\in C^1(D, \\mathbb{R}^N)\\) und \\(x^* \\in D\\) mit \\(F(x^*) = 0_N\\). Falls ein \\(B \\in \\mathbb{R}^{N \\times N}\\) mit \\[\n\\rho(I - BF'(x^*)) &lt; 1\n\\] existiert, dann ist die Fixpunktiteration \\[\nx^{k+1} = \\Phi(x^k)\n\\] mit \\[\n\\Phi(x) = x - BF(x)\n\\] lokal linear konvergent, d. h. es existiert ein \\(\\delta &gt; 0\\), \\(C &gt; 0\\) und \\(\\theta \\in (0,1)\\) mit \\[\n|x^k - x^*| \\leq C\\theta^k |x^* - x^0|\n\\] für alle \\(x^0 \\in B(x^*, \\delta)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Iterationsverfahren für nichtlineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "07-pol-interpol.html",
    "href": "07-pol-interpol.html",
    "title": "7  Polynom-Interpolation",
    "section": "",
    "text": "7.1 Lagrange-Darstellung\nKonstruktion des Interpolationspolynoms Zum Lösen der Interpolationsaufgabe muss der Koeffizientenvektor \\(a = [a₀;a₁; \\ldots ;a_N] \\in \\mathbb{R}^{N+1}\\) bestimmt werden, welcher \\[a₀+a₁ξ_n + a₂ξ_n^2 + \\ldots + a_Nξ_n^N = f_n\\] für \\(n = 0, \\ldots, N\\) genügt. Dies führt auf ein lineares Gleichungssystem \\(Ta = f\\) mit \\(f = [f₀; \\ldots; f_N] \\in \\mathbb{R}^{N+1}\\) und der Vandermonde-Matrix \\[T = (ξ_n^k)_{n,k=0,\\ldots,N} = \\begin{pmatrix} 1 & ξ_0 & ξ_0^2 & \\ldots & ξ_0^N \\\\ 1 & ξ_1 & ξ_1^2 & \\ldots & ξ_1^N \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & ξ_N & ξ_N^2 & \\ldots & ξ_N^N \\end{pmatrix} \\in \\mathbb{R}^{(N+1) \\times (N+1)}.\\] Diese Matrix ist zwar regulär (da die \\(ξ_n\\) paarweise verschieden sind), hat aber in der Regel eine sehr schlechte Kondition. Eine andere Möglichkeit ist die Darstellung über die Lagrange Basispolynome \\(L_k \\in \\mathbb{P}_N\\) mit \\[L_n(t) = \\prod_{\\substack{k=0 \\\\ k \\neq n}}^N \\frac{t - ξ_k}{ξ_n - ξ_k}.\\] Dadruch, dass \\(t-ξ_k\\) für \\(k \\neq n\\) Nullstellen aufweist und \\(ξ_n - ξ_k\\) das Polynom bei \\(t = ξ_n\\) auf \\(1\\) normiert, ergibt sich die Bedingung \\[L_n(ξ_k) = \\begin{cases} 1, & k = n, \\\\ 0, & k \\neq n. \\end{cases}\\] Damit erhalten wir die Lagrange-Darstellung des Interpolationspolynoms \\[P(t) = \\sum_{n=0}^N f_nL_n(t)\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Polynom-Interpolation</span>"
    ]
  },
  {
    "objectID": "07-pol-interpol.html#newton-darstellung",
    "href": "07-pol-interpol.html#newton-darstellung",
    "title": "7  Polynom-Interpolation",
    "section": "7.2 Newton-Darstellung",
    "text": "7.2 Newton-Darstellung\nWir betrachten Stützstellen \\(\\xi_1, \\ldots, \\xi_n\\) und die zugehörigen Funktionswerte \\(f(\\xi_1), \\ldots, f(\\xi_n)\\). Nach Newton-Schema rechnen wir die Koeffizienten \\(c_k\\) aus und erhalten die Newton-Polynome \\(N_k(t)\\):\n\\[\nP_n(t) = \\sum_{k=0}^{n} c_k \\cdot N_k(t) \\quad \\text{mit} \\quad N_k(t) = \\prod_{j=0}^{k-1} (t - \\xi_j)\n\\]\n\n\n\n\n\n\n\n(7.3) Satz\n\n\n\nSei \\(f \\in C^{N+1}[a,b]\\) mit \\(a &lt; b\\) und \\(a \\le t \\le b\\). Zu \\(a \\le \\xi_0 \\le \\xi_1 \\le \\dots \\le \\xi_N \\le b\\) definiere \\[\nf_n = \\frac{1}{e_n!} \\left(\\frac{d}{dt}\\right)^{e_n} f(\\xi_n).\n\\]\nFür den Interpolationsfehler gilt dann \\[\nf(t) - P_N(t) = \\frac{\\omega_{N+1}(t)}{(N+1)!} \\left(\\frac{d}{dt}\\right)^{N+1} f(\\tau_t)\n\\] mit \\(\\tau_t \\in I := [\\min\\{\\xi_0, t\\}, \\max\\{t, \\xi_N\\}]\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Polynom-Interpolation</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Mathematische Grundlagen",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#mathematische-grundlagen",
    "href": "appendix.html#mathematische-grundlagen",
    "title": "Appendix",
    "section": "",
    "text": "Analysis\n\nReihen und Summen\nGeometrische Reihe\nFür alle reellen \\(q \\neq 1\\) und für alle \\(n \\in \\mathbb{N}_0\\) ist:\n\\[\n\\sum_{k=0}^n q^k=\\frac{1-q^{n+1}}{1-q}\n\\]\nDer Grenzwert ist dementsprechend:\n\\[\n\\sum_{k=0}^{\\infty} q^k=\\frac{1}{1-q}\n\\]\n\n\nLogarithmen\n\n\n\n\nLineare Algebra\n\n\n\n\n\n\n\n\n\nflowchart TD\n    all_matrices(\"all matrices\") --&gt; complex(\"complex\") & real(\"real\")\n    real --&gt; non_square(\"non square\") & square(\"square\")\n    square --&gt; general(\"general\") & se_n(\"SE(n)\") & normal(\"normal\")\n    general -- det≠0 --&gt; invertible(\"invertible / regular\")\n    general -- \"det=0\" --&gt; singular(\"singular\")\n    normal --&gt; symmetric(\"symmetric\") & orthogonal(\"orthogonal O(n)\") & skew_symmetric(\"skew-symmetric\")\n    symmetric --&gt; positive_definite(\"positive definite\") & unnamed_sym((\" \"))\n    positive_definite --&gt; diagonal(\"diagonal\") & unnamed_posdef((\" \"))\n    diagonal --&gt; identity(\"identity\") & unnamed_diag((\" \"))\n    orthogonal -- \"det=+1\" --&gt; so_n(\"SO(n)\")\n    orthogonal -- \"det=-1\" --&gt; unnamed_ortho((\" \"))\n\n     all_matrices:::greyNode\n     complex:::greyNode\n     real:::greyNode\n     non_square:::greyNode\n     square:::greyNode\n     general:::greyNode\n     se_n:::blueNode\n     normal:::greyNode\n     invertible:::blueNode\n     singular:::greyNode\n     symmetric:::greyNode\n     orthogonal:::blueNode\n     skew_symmetric:::greyNode\n     positive_definite:::blueNode\n     unnamed_sym:::greyNode\n     diagonal:::blueNode\n     unnamed_posdef:::blueNode\n     identity:::blueNode\n     unnamed_diag:::blueNode\n     so_n:::blueNode\n     unnamed_ortho:::blueNode\n    classDef greyNode fill:#e0e0e0,stroke:#333,stroke-width:2px\n    classDef blueNode fill:#cce6ff,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\nAbbildung 1: Matrix Taxonomie adaptiert von Corke (2023). Matrizen in blau sind invertierbar.\n\n\n\n\nOrthogonale Matrizen\nEine Matrix \\(Q\\) ist orthogonal, wenn:\n\\[\nQ^\\top Q = Q Q^\\top = I\n\\]\n\n\\(Q^\\top = Q^{-1}\\)\n\n\\(Q\\) ist daher auch invertierbar\n\n\\(det(Q) = \\pm 1\\)\n\n\n\nSymmetrische Matrizen\nEine Matrix \\(S\\) ist symmetrisch, wenn:\n\\[\nS^\\top = S\n\\]\n\nDie Eigenwerte von \\(S\\) sind reell\n\nDie Eigenvektoren von \\(S\\) sind orthogonal zueinander\n\n\\(S^\\top S = S S^\\top\\) (normal)\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\), dann ist \\(B=A^\\top A\\) symmetrisch\n\\(S\\) ist nicht notwendigerweise invertierbar\n\n\\(2\\times2\\)-Matrix invertieren\n\\[\nA=\\left(\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right) \\quad \\text { then } \\quad A^{-1}=\\frac{1}{a d-b c}\\left(\\begin{array}{cc}\nd & -b \\\\\n-c & a\n\\end{array}\\right)\n\\]\n\n\n\n\nCorke, Peter. 2023. Robotics, Vision and Control: Fundamental Algorithms in Python. 3. Aufl. Bd. 146. Springer Tracts in Advanced Robotics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-06469-2.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referenzen",
    "section": "",
    "text": "Bartels, Sören. 2016. Numerik 3x9: Drei Themengebiete in\njeweils neun kurzen Kapiteln. 1. Aufl. 2016.\nSpringer-Lehrbuch. Berlin Heidelberg: Springer Spektrum. https://doi.org/10.1007/978-3-662-48203-2.\n\n\nCorke, Peter. 2023. Robotics, Vision and\nControl: Fundamental Algorithms in\nPython. 3rd ed. Vol. 146. Springer Tracts\nin Advanced Robotics. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-06469-2.\n\n\nWeiß, Daniel. 2024. “Numerische Mathematik für\ndie Fachrichtungen Informatik und Ingenieurwesen.”\nKarlsruher Institut für Technologie.\n\n\nWieners, Christian. 2025. “Einführung in die\nNumerische Mathematik.” Karlsruher Institut\nfür Technologie.",
    "crumbs": [
      "Referenzen"
    ]
  }
]