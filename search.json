[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vorlesungsnotizen zu Numerik für Informatiker",
    "section": "",
    "text": "Einführung\nEs handelt sich hierbei um meine Vorlesungsnotizen, basierend auf den Übungsaufzeichnungen, dem offiziellen Skript (Wieners 2025), sowie Passagen aus Bartels (2016).\n\n\n\n\n\n\nVorsicht\n\n\n\nDie Notizen sind nicht vollständig und dienen lediglich als Ergänzung zu den Vorlesungsunterlagen.\nSolltest du einen Fehler finden, kannst du ein Issue anlegen.\n\n\n\n\n\n\nBartels, Sören. 2016. Numerik 3x9: Drei Themengebiete in jeweils neun kurzen Kapiteln. 1. Aufl. 2016. Springer-Lehrbuch. Berlin Heidelberg: Springer Spektrum. https://doi.org/10.1007/978-3-662-48203-2.\n\n\nWieners, Christian. 2025. „Einführung in Die Numerische Mathematik“.",
    "crumbs": [
      "Einführung"
    ]
  },
  {
    "objectID": "01-arithmetik.html",
    "href": "01-arithmetik.html",
    "title": "1  Arithmetik",
    "section": "",
    "text": "1.1 Gleitkommazahlen\nWir betrachten für eine gegebene Basis \\(B \\geq 2\\), einen minimalen Exponent \\(E^{-}\\) und Längen \\(M\\) und \\(E\\) die endliche Menge der normalisierten Gleitpunktzahlen \\(\\mathrm{FL}\\).\n\\[\n\\mathrm{FL}:=\\{ \\pm B^e \\underbrace{\\sum_{l=1}^M a_l B^{-l}}_{=m} \\; | \\; e=E^{-}+\\sum_{k=0}^{E-1} c_k B^k, \\  a_l, c_k \\in\\{0, \\ldots, B-1\\}, \\ a_1 \\neq 0\\} \\cup\\{0\\}\n\\]\nMaschienengenauigkeit\n\\[\n\\text{eps} := \\sup \\left\\{ \\frac{|x - fl(x)|}{|x|} \\ | \\ 1 &lt; x &lt; 2 \\right\\} = \\frac{B^{1-M}}{2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#auslöschung",
    "href": "01-arithmetik.html#auslöschung",
    "title": "1  Arithmetik",
    "section": "1.2 Auslöschung",
    "text": "1.2 Auslöschung\n\nN = 2**10\n\ndef exp(x):\n    \"\"\"\n    Compute the exponential function using Taylor series expansion.\n    \"\"\"\n    return np.sum([x**n / math.factorial(n) for n in range(N)], axis=0)\n\nx = 10\n\nz_bad = exp(-x)\nz_good = 1 / exp(x)\n\nr = np.exp(-x) # reference\n\nnp.abs(z_bad - r) / r, np.abs(z_good - r) / r\n\n(np.float64(6.529424994681785e-09), np.float64(1.4925713791816933e-16))\n\n\nQuadratische Gleichung\nAnstatt \\(x_2=p-\\sqrt{p^2-q}\\), verwenden wir\n\\[\nx_2=p-\\sqrt{p^2-q} \\cdot \\frac{p+\\sqrt{p^2+q}}{p+\\sqrt{p^2+q}} = \\frac{q}{p+\\sqrt{p^2-q}}=\\frac{q}{x_1}\n\\]\n(Satz von Vieta) um die Auslöschung zwischen \\(p\\) und \\(\\sqrt{p^2-q}\\) zu vermeiden.\n\np = 1e10\nq = 1e2\n\nprint(np.roots([1, -2*p, q])) # reference\n\nx1 = p + math.sqrt(p**2 - q)\n\nx2_bad = p - math.sqrt(p**2 - q)\nx2_good = q / x1\n\nx2_bad, x2_good\n\n[2.e+10 5.e-09]\n\n\n(0.0, 5e-09)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#kondition-und-stabilität",
    "href": "01-arithmetik.html#kondition-und-stabilität",
    "title": "1  Arithmetik",
    "section": "1.3 Kondition und Stabilität",
    "text": "1.3 Kondition und Stabilität\n\nDie Kondition eines Problems ist ein Maß dafür, wie stark die Abhängigkeit der Lösung von den Daten ist.\nAbsolute Konditionszahl\n\\[\n\\kappa_\\text{abs}(x) = | f'(x) |\n\\]\nRelative Konditionszahl\n\\[\n\\kappa_\\text{rel}(x) = \\frac{| f'(x) |}{|f(x)|} \\cdot |x|\n\\]\nMatrix Kondition\n\\[\n\\kappa_p(A) = ||A||_p \\cdot ||A^{-1}||_p \\quad \\text{für } p = 1,2,\\infty\n\\]\n\n\n\n\n\n\nHinweis\n\n\n\nFür symmetrische Matrizen (\\(A=A^\\top\\)) gilt:\n\n\\(\\sigma(A) \\subset \\mathbb{R}\\) (Spektrum bzw. alle Eigenwerte sind reell)\n\\(\\|A\\|_2=\\rho(A)\\) (Septralradius bzw. größter Eigenwert im Betrag)\n\\(\\kappa_2(A)=\\frac{\\max _{\\lambda \\in \\sigma(A)}|\\lambda|}{\\min _{\\lambda \\in \\sigma(A)}|\\lambda|}\\) (Verhältnis der größten zur kleinsten Eigenwerte im Betrag)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#vektor--und-matrixnormen",
    "href": "01-arithmetik.html#vektor--und-matrixnormen",
    "title": "1  Arithmetik",
    "section": "1.4 Vektor- und Matrixnormen",
    "text": "1.4 Vektor- und Matrixnormen\nEine Norm auf \\(\\mathbb{R}^n\\) ist eine Abbildung \\(\\|\\cdot\\|: \\mathbb{R}^n \\rightarrow \\mathbb{R}_{\\geq 0}\\) mit den folgenden Eigenschaften:\n\n\\(\\|x\\|=0 \\Longrightarrow x=0\\) für alle \\(x \\in \\mathbb{R}^n\\) (Definitheit);\n\\(\\|x+y\\| \\leq\\|x\\|+\\|y\\|\\) für alle \\(x, y \\in \\mathbb{R}^n\\) (Dreiecksungleichung);\n\\(\\|\\lambda x\\|=|\\lambda|\\|x\\|\\) für alle \\(\\lambda \\in \\mathbb{R}\\) und \\(x \\in \\mathbb{R}^n\\) (Homogenität).\n\nWir verwenden für \\(x \\in \\mathbb{R}^N\\) und \\(A \\in \\mathbb{R}^{M \\times N}\\)\n\\[\n\\begin{array}{ll}\n|x|_1=\\sum_{n=1}^N\\left|x_n\\right| & \\text { 1-Norm } \\\\\n|x|_2=\\sqrt{x^T x}=\\left(\\sum_{n=1}^N\\left|x_n\\right|^2\\right)^{\\frac{1}{2}} & \\text { Euklidische Norm } \\\\\n|x|_{\\infty}=\\max _{n=1, \\ldots, N}\\left|x_n\\right| & \\text { Supremumsnorm }\n\\end{array}\n\\]\nFür Matrizen \\(A \\in \\mathbb{R}^{M \\times N}\\) definieren wir eine allgemeine Norm mit\n\\[\\|A\\|_{p}=\\sup _{x \\in \\mathbb{R}^n,\\|x\\|=1}\\|A x\\|=\\inf \\left\\{c \\geq 0: \\forall x \\in \\mathbb{R}^n\\|A x\\| \\leq c\\|x\\|\\right\\}\\]\nund spezifizieren sie für \\(p=1,2,\\infty,F\\) wie folgt\n\\[\n\\begin{array}{ll}\n\\|A\\|_1=\\max _{n=1, \\ldots, N} \\sum_{m=1}^M|A[m, n]| & \\text { Spaltensummennorm, } \\\\\n\\|A\\|_2=\\sqrt{\\rho\\left(A^T A\\right)} & \\text { Spektralnorm, } \\\\\n\\|A\\|_{\\infty}=\\max _{m=1, \\ldots, M} \\sum_{n=1}^N|A[m, n]| & \\text { Zeilensummennorm, } \\\\\n\\|A\\|_F=\\left(\\sum_{m=1}^M \\sum_{n=1}^N A[m, n]^2\\right)^{\\frac{1}{2}} & \\text { Frobeniusnorm. }\n\\end{array}\n\\]\nDabei ist\n\\[\n\\begin{aligned}\n& \\rho(A)=\\max \\{|\\lambda|: \\lambda \\in \\sigma(A)\\}  \\text { Spektralradius, }  \\\\\n& \\sigma(A)=\\left\\{\\lambda \\in \\mathbb{C}: \\operatorname{det}\\left(A-\\lambda I_N\\right)=0\\right\\}  \\text { Spektrum. }\n\\end{aligned}\n\\]\nEs gilt immer\n\\[|A x|_p \\leq\\|A\\|_p|x|_p\\]\nfür alle \\(x \\in \\mathbb{R}^N\\) und wegen \\(\\|A\\|_2 \\leq\\|A\\|_F\\) auch\n\\[\n|A x|_2 \\leq\\|A\\|_2|x|_2 \\leq\\|A\\|_F|x|_2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "02-lgs.html",
    "href": "02-lgs.html",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "",
    "text": "2.1 Vorwärts-Substitution\n\\[\nL = \\begin{pmatrix}\n1      \\\\\nl_{21} & 1      \\\\\nl_{31} & l_{32} & 1      \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\nl_{n1} & l_{n2} & l_{n3} & \\dots  & 1 \\\\\n\\end{pmatrix}\n\\]\nDie Vorwärts-Substitution löst \\(L\\cdot\\mathbf{y}=\\mathbf{b}\\) (normierte untere Dreiecksmatrix), indem wir über die Zeilen iterieren und dabei die Lösungen der vorheringen \\(\\mathbf{x}_j\\) für die Berechung des aktuellen \\(\\mathbf{x}_i\\) verwenden (\\(\\mathbf{x}_1 = \\mathbf{b}_1\\)).\nDie Laufzeit liegt somit in \\(O(n^2)\\).\ndef forward_sub(lower, rhs):\n    n = lower.shape[0]\n    solution = np.zeros(n)\n    for i in range(n):\n        solution[i] = rhs[i]\n        for j in range(i):\n            solution[i] -= lower[i, j] * solution[j]\n            solution[i] = solution[i] / lower[i, i]\n    return solution\nforward_sub(np.array([\n    [1, 0, 0], \n    [2, 1, 0], \n    [3, 4, 1]]\n), np.array([1, 2, 3]))\n\narray([1., 0., 0.])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#rückwärts-substitution",
    "href": "02-lgs.html#rückwärts-substitution",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.2 Rückwärts-Substitution",
    "text": "2.2 Rückwärts-Substitution\n\\[\nR = \\begin{pmatrix}\nr_{11} & r_{12} & r_{13} & \\dots  & r_{1n} \\\\\n    & r_{22} & r_{23} & \\dots  & r_{2n} \\\\\n    &        & r_{33} & \\dots  & r_{3n} \\\\\n    &        &        & \\ddots & \\vdots \\\\\n    &        &        &        & r_{nn}\n\\end{pmatrix}\n\\]\nDie Rückwärts-Substitution löst \\(R \\cdot \\mathbf{x}=\\mathbf{y}\\), indem wir von der letzten Zeile aus das verfahren der Vorwärts-Substitution anwenden.\nDie Laufzeit liegt somit ebenfalls in \\(O(n^2)\\).\n\ndef backward_sub(upper, rhs):\n    n = upper.shape[0]\n    solution = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        tmp = rhs[i]\n        for j in range(i + 1, n):\n            tmp -= upper[i, j] * solution[j]\n            solution[i] = tmp / upper[i, i]\n    return solution\n\n\nbackward_sub(np.array([\n    [2, 2, 3], \n    [0, 1, 4], \n    [0, 0, 1]]\n), np.array([1, 0, 0]))\n\narray([0.5, 0. , 0. ])",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#lr-zerlegung",
    "href": "02-lgs.html#lr-zerlegung",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.3 LR-Zerlegung",
    "text": "2.3 LR-Zerlegung\n(en. LU-Decomposition)\n\n\n\n\n\n\n\nWarnung\n\n\n\nDie \\(1\\)-en auf der Diagonalen der \\(L\\)-Matrix bleiben beim Zeilentauschen unverändert\n\n\nDie \\(LR\\)-Zerlegung lässt sich mittels des Gauß-Algorithmus bestimmen, indem wir \\(A\\) auf eine untere Dreiecksmatrix \\(R\\) gaußen und uns die Operationen in \\(L\\) “merken”. Sie ist eindeutig und benötigt \\(O(n^3)\\) Operationen.\nDie Berechnung ist nicht stabil.\nHinreichende Bedingungen für die Exsistenz einer \\(LR\\)-Zerlegung für eine quadratische Matrix \\(A\\):\n\nstrikt diagonal-dominant, daher das Diagonalelement ist größer als die Summe aller anderen Elemente in der Zeile, bzw.\n\n\\[\n|A[n, n]|&gt;\\sum_{\\substack{k=1 \\\\ k \\neq n}}^N|A[n, k]| \\quad \\text { für } n=1, \\ldots, N\n\\]\n\npositiv definit, daher alle Eigenwerte \\(&gt; 0\\), bzw.\n\n\\[\nx^{\\top} A x&gt;0 \\quad \\text { für alle } x \\in \\mathbb{R}^N, x \\neq 0 .\n\\]\n\n\n\n\n\n\nHinweis\n\n\n\n\n\n\nHauptminorenkriterium für positiv definite Matrizen\n\n\n\n\nFalls diese Bedingungen nicht gegeben sind, können wir mittels Zeilenvertauschung (Permutationsmatrix \\(P\\)) eine LR-zerlegbare Matrix \\(PA\\) in \\(O(n^3)\\) erzeugen.\n\ndef lu_decomposition(matrix):\n    n = matrix.shape[0]\n    lower = np.zeros(shape=matrix.shape)\n    upper = np.zeros(shape=matrix.shape)\n    for j in range(n):\n        lower[j][j] = 1.0\n        for i in range(j + 1):\n            first_sum = sum(upper[k][j] * lower[i][k] for k in range(i))\n            upper[i][j] = matrix[i][j] - first_sum\n        for i in range(j, n):\n            second_sum = sum(upper[k][j] * lower[i][k] for k in range(j))\n            lower[i][j] = (matrix[i][j] - second_sum) / upper[j][j]\n    return lower, upper\n\ndef solve_with_lu(matrix, rhs):\n    lower, upper = lu_decomposition(matrix)\n    y = forward_sub(lower, rhs)\n    return backward_sub(upper, y)\n\n\nmatrix = np.array([[2.0, 1.0],\n[1.0, 4.0]])\nrhs = np.array([1.0, 2.0])\nsolution = solve_with_lu(matrix, rhs)\nprint(\"solution\", solution)\ntest = rhs - np.dot(matrix, solution)\nprint(\"test \",test)\n\nsolution [0.5 0. ]\ntest  [0.  1.5]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#choelsky-zerlegung",
    "href": "02-lgs.html#choelsky-zerlegung",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.4 Choelsky-Zerlegung",
    "text": "2.4 Choelsky-Zerlegung\n\n\n\n\n\n\n(2.7) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) symmetrisch und positiv definit. Dann existiert genau eine Cholesky-Zerlegung \\(A=L L^{\\top}\\) mit einer regulären unteren Dreiecksmatrix \\(L\\).\n\n\nEs handelt sich somit um eine Spezialisierung der LR-Zerlegung für symmetrisch, positiv definite Matrizen.\n\\(\\renewcommand{\\b}[1]{\\color{teal}{#1}\\color{black}}\\renewcommand{\\o}[1]{\\color{orange}{#1}\\color{black}}\\)\n\\[\\begin{align}\nA &=\n\\begin{pmatrix}\n   a_{11} & a_{21} & a_{31}\\\\\n   a_{21} & a_{22} & a_{32}\\\\\n   a_{31} & a_{32} & a_{33}\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n   l_{11} & 0 & 0 \\\\\n   l_{21} & l_{22} & 0 \\\\\n   l_{31} & l_{32} & l_{33}\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n   l_{11} & l_{21} & l_{31} \\\\\n   0 & l_{22} & l_{32} \\\\\n   0 & 0 & l_{33}\n\\end{pmatrix} \\equiv L L^T \\\\\n&= \\begin{pmatrix}\n   l_{11}^2     & l_{21}l_{11} & l_{31}l_{11} \\\\\n   l_{21}l_{11} & l_{21}^2 + l_{22}^2& l_{31}l_{21}+l_{32}l_{22} \\\\\n   l_{31}l_{11} & l_{31}l_{21}+l_{32}l_{22} & l_{31}^2 + l_{32}^2+l_{33}^2\n\\end{pmatrix}\\end{align}\\]\n\n2.4.1 Berechnung\nDiagonalelemente:\n\\[l_{kk} = \\sqrt{a_{kk} - \\sum_{\\b{j}=1}^{k-1} l_{k\\b{j}}^2}\\]\nRest:\n\\[l_{\\o{i}k} = \\frac{1}{l_{kk}} \\left ( a_{ik} - \\sum_{\\b{j}=1}^{k-1} l_{\\o{i}\\b{j}}l_{k\\b{j}} \\right )\\]\n\ndef cholesky_decomposition(A):\n    n = matrix.shape[0]\n    lower = np.zeros(matrix.shape)\n    lower[0, 0] = np.sqrt(matrix[0, 0])\n    for n in range(1, n):\n        y = forward_sub(lower[:n, :n], matrix[n, :n]) # linalg.solve_triangular(lower[:n, :n], matrix[n, :n], lower=True)\n        lower[n, :n] = y\n        lower[n, n] = np.sqrt(matrix[n, n] - np.dot(y, y))\n    return lower\n\ndef solve_with_cholesky(matrix, rhs):\n    lower = cholesky_decomposition(matrix)\n    y = forward_sub(lower, rhs)\n    return backward_sub(lower.transpose(), y)\n\n\nmatrix = np.array([[2.0, 1.0],\n[1.0, 4.0]])\nrhs = np.array([1.0, 2.0])\nrhs = np.array([1.0, 2.0])\nsolution = solve_with_cholesky(matrix, rhs)\nprint(\"solution\",solution)\ntest = rhs - np.dot(matrix, solution)\nprint(\"test \",test)\n\nsolution [0.70710678 0.        ]\ntest  [-0.41421356  1.29289322]\n\n\n\nDie Cholesky-Zerlegung ist stabil: Es gilt \\(\\kappa_2(L)^2=\\kappa(A)\\)\nDie Berechnung der Cholesky-Zerlegung benötigt nur halbsoviele Operationen wie die Berechnung einer LR-Zerlegung.\nMatrizen mit einer geeigneten Hüllenstruktur (viele Nullelemente wie bei der Bandmatrix) können effizienter gelöst werden (Bandmatrix in \\(O(NM^2)\\))\n\n\n\n\nSchematische Darstellung einer Bandmatrix",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#qr-zerlegung",
    "href": "02-lgs.html#qr-zerlegung",
    "title": "2  Direkte Lösungsverfahren für lineare Gleichungen",
    "section": "2.5 QR-Zerlegung",
    "text": "2.5 QR-Zerlegung\n\n\n\n\n\n\n(2.14) Satz (QR-Zerlegung)\n\n\n\nZu \\(A \\in \\mathbb{R}^{M \\times N}\\) existiert eine \\(Q R\\)-Zerlegung \\(A=Q R\\) in eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{M \\times M}\\) mit \\(Q^{\\top} Q=I_M\\) und eine obere Dreiecksmatrix \\(R \\in \\mathbb{R}^{M \\times N}\\) mit \\(R[m, n]=0\\) für \\(m&gt;n\\).\n\n\n\nDas LGS \\(Ax = b\\) kann durch die Berechnung \\(y = Q^\\top b\\) und darauf mit Rücksubstitution \\(Rx = y\\) gelöst werden.\nAsymptotischer Aufwand in \\(O(N^3)\\)\n\nRotationen und Drehungen sind orthogonale Matrizen \\(Q \\in \\mathbb{R}^{N \\times N}\\) mit\n\n\\(Q Q^{\\top}=I_N, \\ Q^{\\top} Q=I_N\\), so dass \\(Q^{-1}=Q^{\\top}\\),\n\\(|Q v|_2=|v|_2\\) und \\((Q v)^{\\top}(Q w)=v^{\\top} w\\) Längen und Winkel erhaltend,\n\\(\\kappa_2(Q)=1\\).\n\n\n2.5.1 Householder Transformationen\nWir erhalten \\(N\\) orthogonale Vektoren für \\(A\\), indem wir den ersten Spaltenvektor \\(v_1\\) mittels einer Householder Transformation (Spiegelung) \\(Q_1\\) auf die \\(x\\)-Achse (\\(e_1\\)) abbilden und dies sukzessiv für die nächsten Spaltenvektoren \\(v_i\\) aus \\(Q_1 \\cdot \\dots \\cdot Q_{i-1} \\cdot A\\) wiederholen (dabei vernachlässigen wir die ersten \\(i\\) Zeilen, da wir nur einen Untervektorraum in \\(\\mathbb{R}^{N-i}\\) betrachten).\n\n\n\nQR-Zerlegung mittels Householder Transformationen berechnen\n\n\n\n\n2.5.2 Givens-Rotation\nAlternativ können wir Givens-Rotationen verwenden, um die Matrix \\(A\\) in eine obere Dreiecksmatrix zu überführen. Eine Givens-Rotation ist eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{N \\times N}\\), die die \\(m\\)-te und \\(n\\)-te Zeile von \\(A\\) rotiert, um die Elemente unterhalb der Hauptdiagonalen zu eliminieren.\n\n\n\nQR-Zerlegung mittels Givens-Rotation\n\n\nBesonders bei dünnbesetzten Matrizen (sparse) ist die Givens-Rotation effizient.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Direkte Lösungsverfahren für lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "03-fitting.html",
    "href": "03-fitting.html",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "",
    "text": "3.1 Normalengleichung\nAnstatt eine extakte Lösung zu finden, können wir auch eine Näherungslösung suchen, die die Summe der Abweichungen minimiert. Dies wird als lineare Ausgleichsrechnung bezeichnet.\n\\[\nA^\\top Ax = A^\\top b\n\\]\nist äquivalent zur Minimierung von \\(\\|Ax - b\\|_2\\)\nLösungsverfahren:\nFalls \\(A^\\top A\\) invertierbar und gut konditioniert:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#normalengleichung",
    "href": "03-fitting.html#normalengleichung",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "",
    "text": "Wenn \\(A^\\top A\\) invertierbar ist, ist die Lösung eindeutig.\nWenn \\(\\operatorname{Kern} \\{v \\in \\mathbb{R}^N : Av=0\\} \\neq \\{0\\}\\), dann ist die Lösung nicht eindeutig. In diesem Fall ist das Problem nicht sachgemäß gestellt!\n\n\n\n\nBerechne \\(QR\\)-Zerlegung von \\(A=QR\\)\n\n\\(R[1:N, 1:N]x = (Q^T b)[1:N]\\) lösen\n\nBerechne \\(A^\\top A\\) mittels Cholesky-Zerlegung",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#singularitätszerlegung",
    "href": "03-fitting.html#singularitätszerlegung",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "3.2 Singularitätszerlegung",
    "text": "3.2 Singularitätszerlegung\n(en. Singular Value Decomposition, SVD)\n\nFalls \\(A\\) singulär (nicht invertierbar) oder schlecht konditioniert ist, kann die Singularitätszerlegung verwendet werden um die Normalengleichung zu lösen.\n\n\n\n\n\n\n(3.4) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{K \\times N}\\) mit \\(R = \\text{rang} A\\). Dann existieren Singulärwerte \\(\\sigma_1, \\dots, \\sigma_R &gt; 0\\), Matrizen \\(V \\in \\mathbb{R}^{K \\times R}, U \\in \\mathbb{R}^{N \\times R}\\) mit \\(V^\\top V = I_R = U^\\top U\\) und eine Zerlegung\n\\[\nA = U \\Sigma V^\\top\n\\]\nmit \\(\\Sigma = \\text{diag}(\\sigma_1, \\dots, \\sigma_R)= \\text{diag}(\\sqrt{\\lambda_1}, \\dots, \\sqrt{\\lambda_R}) \\in \\mathbb{R}^{R \\times R}\\).\n\n\nWir führen eine Drehung mit \\(V^\\top\\) durch, skalieren mit den Singularitätswerten und passen die Dimension von \\(N\\) auf \\(K\\) mit \\(\\Sigma\\) und drehen dann in \\(\\mathbb{R}^K\\) mittels \\(U\\).\n\n\\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_R &gt; 0\\) sind die geordneten Eigenwerte von \\(A^\\top A\\) und \\(A A^\\top\\).\n\\(U\\) enthält die Eigenvektoren von \\(A A^\\top\\) als Spaltenvektoren und \\(V\\) die Eigenvektoren von \\(A^\\top A\\) als Zeilenvektoren (transponiert).\nEs gilt \\(A = \\sum_{k=1}^R \\sigma_k v_k u_k^\\top\\).\nDie Spalten \\(v_1,...,v_R\\) von V bilden eine ONB von \\(\\operatorname{Bild} A\\).\nDie Spalten \\(u_1,...,u_R\\) von U lassen sich mit \\(u_{R+1},...,u_N\\) zu einer ONB von \\(\\mathbb{R}^N\\) ergänzen. Dann ist \\(u_{R+1},...,u_N\\) eine ONB von \\(\\operatorname{Kern} A\\).\n\\(A^+ := \\sum_{k=1}^R \\frac{1}{\\sigma_k} u_k v_k^T = U \\Sigma^{-1} V^T\\) heißt Pseudo-Inverse.\nIst \\(A\\) regulär, so gilt \\(A^{-1} = A^+\\). Die Normalengleichung und damit \\(\\|Ax-b\\|_2 = \\min!\\) wird durch \\(x = A^+b\\) gelöst.\n\\(\\|A\\|_2 = \\sigma_1\\) und \\(\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_R}\\). Berechnung aufwendig, erfordert Eigenwertberechnung von \\(S = A^T A\\). Stabile Berechnung mit \\(O(N^3)\\) Operationen!\nDie Berechnung von \\(A^+b\\) ist für \\(\\sigma_R \\ll 1\\) numerisch nicht stabil.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#tikhonov-regularisierung",
    "href": "03-fitting.html#tikhonov-regularisierung",
    "title": "3  Lineare Ausgleichsrechnung",
    "section": "3.3 Tikhonov-Regularisierung",
    "text": "3.3 Tikhonov-Regularisierung\nStabilisiert die Singularitätszerlegung, indem ein Regularisierungsterm hinzugefügt wird.\n\n\n\n\n\n\n(3.5) Satz\n\n\n\nZu \\(A \\in \\mathbb{R}^{M \\times N}, b \\in \\mathbb{R}^M, \\alpha &gt; 0\\) existiert genau ein \\(x^\\alpha \\in \\mathbb{R}^N\\), das die Tikhonov-Regularisierung \\[\nF_\\alpha(x) := \\frac{1}{2}|Ax-b|_2^2 + \\frac{\\alpha}{2}|x|_2^2\n\\] minimiert. Es gilt \\[\nx^\\alpha = (A^\\top A + \\alpha I_N)^{-1} A^\\top b.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html",
    "href": "04-eigenwerte.html",
    "title": "4  Eigenwertberechnung",
    "section": "",
    "text": "4.0.1 Hessenberg-Matrix\n\\[\nH = \\begin{pmatrix}\n* & * & * & * & * \\\\\n* & * & * & * & * \\\\\n0 & * & * & * & * \\\\\n0 & 0 & * & * & * \\\\\n0 & 0 & 0 & * & *\n\\end{pmatrix}\n\\]\nEine Matrix \\(H \\in \\mathbb{R}^{N \\times N}\\) heißt (obere) Hessenberg-Matrix, wenn \\(H[n+2:N, n] = 0_{N-n-1}\\) für \\(n = 1,...,N-2\\) gilt.\n\n\n\n\n\n\n(4.3) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\). Dann existiert eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{N \\times N}\\), so dass \\[\nH = QAQ^{\\top}\n\\] eine Hessenberg-Matrix ist.\nDie Berechnung von \\(Q\\) benötigt \\(O(N^3)\\) Operationen.\n\n\n\n\n\nBerechnung der Hessenberg-Matrix\n\n\n\nA = np.array([\n    [8, 12/5, -9/5],\n    [12/5, 109/25, 12/25],\n    [-9/5, 12/25, 116/25]\n])\n\nprint(scipy.linalg.hessenberg(A))\n\n[[8 -3 0]\n [-3 4 0]\n [0 0 5]]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Mathematische Grundlagen",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#mathematische-grundlagen",
    "href": "appendix.html#mathematische-grundlagen",
    "title": "Appendix",
    "section": "",
    "text": "Analysis\n\nReihen und Summen\nGeometrische Reihe\nFür alle reellen \\(q \\neq 1\\) und für alle \\(n \\in \\mathbb{N}_0\\) ist:\n\\[\n\\sum_{k=0}^n q^k=\\frac{1-q^{n+1}}{1-q}\n\\]\nDer Grenzwert ist dementsprechend:\n\\[\n\\sum_{k=0}^{\\infty} q^k=\\frac{1}{1-q}\n\\]\n\n\n\nLineare Algebra\n\n\n\n\n\n\n\n\n\nflowchart TD\n    all_matrices(\"all matrices\") --&gt; complex(\"complex\") & real(\"real\")\n    real --&gt; non_square(\"non square\") & square(\"square\")\n    square --&gt; general(\"general\") & se_n(\"SE(n)\") & normal(\"normal\")\n    general -- det≠0 --&gt; invertible(\"invertible / regular\")\n    general -- \"det=0\" --&gt; singular(\"singular\")\n    normal --&gt; symmetric(\"symmetric\") & orthogonal(\"orthogonal O(n)\") & skew_symmetric(\"skew-symmetric\")\n    symmetric --&gt; positive_definite(\"positive definite\") & unnamed_sym((\" \"))\n    positive_definite --&gt; diagonal(\"diagonal\") & unnamed_posdef((\" \"))\n    diagonal --&gt; identity(\"identity\") & unnamed_diag((\" \"))\n    orthogonal -- \"det=+1\" --&gt; so_n(\"SO(n)\")\n    orthogonal -- \"det=-1\" --&gt; unnamed_ortho((\" \"))\n\n     all_matrices:::greyNode\n     complex:::greyNode\n     real:::greyNode\n     non_square:::greyNode\n     square:::greyNode\n     general:::greyNode\n     se_n:::blueNode\n     normal:::greyNode\n     invertible:::blueNode\n     singular:::greyNode\n     symmetric:::greyNode\n     orthogonal:::blueNode\n     skew_symmetric:::greyNode\n     positive_definite:::blueNode\n     unnamed_sym:::greyNode\n     diagonal:::blueNode\n     unnamed_posdef:::blueNode\n     identity:::blueNode\n     unnamed_diag:::blueNode\n     so_n:::blueNode\n     unnamed_ortho:::blueNode\n    classDef greyNode fill:#e0e0e0,stroke:#333,stroke-width:2px\n    classDef blueNode fill:#cce6ff,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\nAbbildung 1: Matrix Taxonomie adaptiert von Corke (2023). Matrizen in blau sind invertierbar.\n\n\n\n\\(2\\times2\\)-Matrix invertieren\n\\[\nA=\\left(\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right) \\quad \\text { then } \\quad A^{-1}=\\frac{1}{a d-b c}\\left(\\begin{array}{cc}\nd & -b \\\\\n-c & a\n\\end{array}\\right)\n\\]\n\n\n\n\nCorke, Peter. 2023. Robotics, Vision and Control: Fundamental Algorithms in Python. 3. Aufl. Bd. 146. Springer Tracts in Advanced Robotics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-06469-2.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referenzen",
    "section": "",
    "text": "Bartels, Sören. 2016. Numerik 3x9: Drei Themengebiete in\njeweils neun kurzen Kapiteln. 1. Aufl. 2016.\nSpringer-Lehrbuch. Berlin Heidelberg: Springer Spektrum. https://doi.org/10.1007/978-3-662-48203-2.\n\n\nCorke, Peter. 2023. Robotics, Vision and\nControl: Fundamental Algorithms in\nPython. 3rd ed. Vol. 146. Springer Tracts\nin Advanced Robotics. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-06469-2.\n\n\nWieners, Christian. 2025. “Einführung in Die\nNumerische Mathematik.”",
    "crumbs": [
      "Referenzen"
    ]
  }
]