[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vorlesungsnotizen zu Numerik f√ºr Informatiker",
    "section": "",
    "text": "Einf√ºhrung\nEs handelt sich hierbei um meine Vorlesungsnotizen, basierend auf den √úbungsaufzeichnungen, dem offiziellen Skript von Wieners (2025), sowie Passagen aus Bartels (2016) und dem Skript von Wei√ü (2024).\n\n\n\n\n\n\nVorsicht\n\n\n\nDie Notizen sind nicht vollst√§ndig und dienen lediglich als Erg√§nzung zu den Vorlesungsunterlagen.\nSolltest du einen Fehler finden, kannst du ein Issue anlegen.\n\n\n\n\n\n\nBartels, S√∂ren. 2016. Numerik 3x9: Drei Themengebiete in jeweils neun kurzen Kapiteln. 1. Aufl. 2016. Springer-Lehrbuch. Berlin Heidelberg: Springer Spektrum. https://doi.org/10.1007/978-3-662-48203-2.\n\n\nWei√ü, Daniel. 2024. ‚ÄûNumerische Mathematik f√ºr die Fachrichtungen Informatik und Ingenieurwesen‚Äú. Karlsruher Institut f√ºr Technologie.\n\n\nWieners, Christian. 2025. ‚ÄûEinf√ºhrung in die Numerische Mathematik‚Äú. Karlsruher Institut f√ºr Technologie.",
    "crumbs": [
      "Einf√ºhrung"
    ]
  },
  {
    "objectID": "01-arithmetik.html",
    "href": "01-arithmetik.html",
    "title": "1¬† Arithmetik",
    "section": "",
    "text": "1.1 Gleitkommazahlen\nWir betrachten f√ºr eine gegebene Basis \\(B \\geq 2\\), einen minimalen Exponent \\(E^{-}\\) und L√§ngen \\(M\\) und \\(E\\) die endliche Menge der normalisierten Gleitpunktzahlen \\(\\mathrm{FL}\\).\n\\[\n\\mathrm{FL}:=\\{ \\pm B^e \\underbrace{\\sum_{l=1}^M a_l B^{-l}}_{=m} \\; | \\; e=E^{-}+\\sum_{k=0}^{E-1} c_k B^k, \\  a_l, c_k \\in\\{0, \\ldots, B-1\\}, \\ a_1 \\neq 0\\} \\cup\\{0\\}\n\\]\nMaschienengenauigkeit\n\\[\n\\text{eps} := \\sup \\left\\{ \\frac{|x - fl(x)|}{|x|} \\ | \\ 1 &lt; x &lt; 2 \\right\\} = \\frac{B^{1-M}}{2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#ausl√∂schung",
    "href": "01-arithmetik.html#ausl√∂schung",
    "title": "1¬† Arithmetik",
    "section": "1.2 Ausl√∂schung",
    "text": "1.2 Ausl√∂schung\n\nN = 2**10\n\ndef exp(x):\n    \"\"\"\n    Compute the exponential function using Taylor series expansion.\n    \"\"\"\n    return np.sum([x**n / math.factorial(n) for n in range(N)], axis=0)\n\nx = 10\n\nz_bad = exp(-x)\nz_good = 1 / exp(x)\n\nr = np.exp(-x) # reference\n\nnp.abs(z_bad - r) / r, np.abs(z_good - r) / r\n\n(np.float64(6.529424994681785e-09), np.float64(1.4925713791816933e-16))\n\n\nQuadratische Gleichung\nAnstatt \\(x_2=p-\\sqrt{p^2-q}\\), verwenden wir\n\\[\nx_2=p-\\sqrt{p^2-q} \\cdot \\frac{p+\\sqrt{p^2+q}}{p+\\sqrt{p^2+q}} = \\frac{q}{p+\\sqrt{p^2-q}}=\\frac{q}{x_1}\n\\]\n(Satz von Vieta) um die Ausl√∂schung zwischen \\(p\\) und \\(\\sqrt{p^2-q}\\) zu vermeiden.\n\np = 1e10\nq = 1e2\n\nprint(np.roots([1, -2*p, q])) # reference\n\nx1 = p + math.sqrt(p**2 - q)\n\nx2_bad = p - math.sqrt(p**2 - q)\nx2_good = q / x1\n\nx2_bad, x2_good\n\n[2.e+10 5.e-09]\n\n\n(0.0, 5e-09)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#kondition-und-stabilit√§t",
    "href": "01-arithmetik.html#kondition-und-stabilit√§t",
    "title": "1¬† Arithmetik",
    "section": "1.3 Kondition und Stabilit√§t",
    "text": "1.3 Kondition und Stabilit√§t\n\nDie Kondition eines Problems ist ein Ma√ü daf√ºr, wie stark die Abh√§ngigkeit der L√∂sung von den Daten ist.\nAbsolute Konditionszahl\n\\[\n\\kappa_\\text{abs}(x) = | f'(x) |\n\\]\nRelative Konditionszahl\n\\[\n\\kappa_\\text{rel}(x) = \\frac{| f'(x) |}{|f(x)|} \\cdot |x|\n\\]\nMatrix Kondition\n\\[\n\\kappa_p(A) = ||A||_p \\cdot ||A^{-1}||_p \\quad \\text{f√ºr } p = 1,2,\\infty\n\\]\n\n\n\n\n\n\nHinweis\n\n\n\nF√ºr symmetrische Matrizen (\\(A=A^\\top\\)) gilt:\n\n\\(\\sigma(A) \\subset \\mathbb{R}\\) (Spektrum bzw. alle Eigenwerte sind reell)\n\\(\\|A\\|_2=\\rho(A)\\) (Septralradius bzw. gr√∂√üter Eigenwert im Betrag)\n\\(\\kappa_2(A)=\\frac{\\max _{\\lambda \\in \\sigma(A)}|\\lambda|}{\\min _{\\lambda \\in \\sigma(A)}|\\lambda|}\\) (Verh√§ltnis der gr√∂√üten zur kleinsten Eigenwerte im Betrag)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "01-arithmetik.html#vektor--und-matrixnormen",
    "href": "01-arithmetik.html#vektor--und-matrixnormen",
    "title": "1¬† Arithmetik",
    "section": "1.4 Vektor- und Matrixnormen",
    "text": "1.4 Vektor- und Matrixnormen\nEine Norm auf \\(\\mathbb{R}^n\\) ist eine Abbildung \\(\\|\\cdot\\|: \\mathbb{R}^n \\rightarrow \\mathbb{R}_{\\geq 0}\\) mit den folgenden Eigenschaften:\n\n\\(\\|x\\|=0 \\Longrightarrow x=0\\) f√ºr alle \\(x \\in \\mathbb{R}^n\\) (Definitheit);\n\\(\\|x+y\\| \\leq\\|x\\|+\\|y\\|\\) f√ºr alle \\(x, y \\in \\mathbb{R}^n\\) (Dreiecksungleichung);\n\\(\\|\\lambda x\\|=|\\lambda|\\|x\\|\\) f√ºr alle \\(\\lambda \\in \\mathbb{R}\\) und \\(x \\in \\mathbb{R}^n\\) (Homogenit√§t).\n\nWir verwenden f√ºr \\(x \\in \\mathbb{R}^N\\) und \\(A \\in \\mathbb{R}^{M \\times N}\\)\n\\[\n\\begin{array}{ll}\n|x|_1=\\sum_{n=1}^N\\left|x_n\\right| & \\text { 1-Norm } \\\\\n|x|_2=\\sqrt{x^T x}=\\left(\\sum_{n=1}^N\\left|x_n\\right|^2\\right)^{\\frac{1}{2}} & \\text { Euklidische Norm } \\\\\n|x|_{\\infty}=\\max _{n=1, \\ldots, N}\\left|x_n\\right| & \\text { Supremumsnorm }\n\\end{array}\n\\]\nF√ºr Matrizen \\(A \\in \\mathbb{R}^{M \\times N}\\) definieren wir eine allgemeine Norm mit\n\\[\\|A\\|_{p}=\\sup _{x \\in \\mathbb{R}^n,\\|x\\|=1}\\|A x\\|=\\inf \\left\\{c \\geq 0: \\forall x \\in \\mathbb{R}^n\\|A x\\| \\leq c\\|x\\|\\right\\}\\]\nund spezifizieren sie f√ºr \\(p=1,2,\\infty,F\\) wie folgt\n\\[\n\\begin{array}{ll}\n\\|A\\|_1=\\max _{n=1, \\ldots, N} \\sum_{m=1}^M|A[m, n]| & \\text { Spaltensummennorm, } \\\\\n\\|A\\|_2=\\sqrt{\\rho\\left(A^T A\\right)} & \\text { Spektralnorm, } \\\\\n\\|A\\|_{\\infty}=\\max _{m=1, \\ldots, M} \\sum_{n=1}^N|A[m, n]| & \\text { Zeilensummennorm, } \\\\\n\\|A\\|_F=\\left(\\sum_{m=1}^M \\sum_{n=1}^N A[m, n]^2\\right)^{\\frac{1}{2}} & \\text { Frobeniusnorm. }\n\\end{array}\n\\]\nDabei ist\n\\[\n\\begin{aligned}\n& \\rho(A)=\\max \\{|\\lambda|: \\lambda \\in \\sigma(A)\\}  \\text { Spektralradius, }  \\\\\n& \\sigma(A)=\\left\\{\\lambda \\in \\mathbb{C}: \\operatorname{det}\\left(A-\\lambda I_N\\right)=0\\right\\}  \\text { Spektrum. }\n\\end{aligned}\n\\]\nEs gilt immer\n\\[|A x|_p \\leq\\|A\\|_p|x|_p\\]\nf√ºr alle \\(x \\in \\mathbb{R}^N\\) und wegen \\(\\|A\\|_2 \\leq\\|A\\|_F\\) auch\n\\[\n|A x|_2 \\leq\\|A\\|_2|x|_2 \\leq\\|A\\|_F|x|_2\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Arithmetik</span>"
    ]
  },
  {
    "objectID": "02-lgs.html",
    "href": "02-lgs.html",
    "title": "2¬† Direkte L√∂sungsverfahren f√ºr lineare Gleichungen",
    "section": "",
    "text": "2.1 Vorw√§rts-Substitution\n\\[\nL = \\begin{pmatrix}\n1      \\\\\nl_{21} & 1      \\\\\nl_{31} & l_{32} & 1      \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\nl_{n1} & l_{n2} & l_{n3} & \\dots  & 1 \\\\\n\\end{pmatrix}\n\\]\nDie Vorw√§rts-Substitution l√∂st \\(L\\cdot\\mathbf{y}=\\mathbf{b}\\) (normierte untere Dreiecksmatrix), indem wir √ºber die Zeilen iterieren und dabei die L√∂sungen der vorheringen \\(\\mathbf{x}_j\\) f√ºr die Berechung des aktuellen \\(\\mathbf{x}_i\\) verwenden (\\(\\mathbf{x}_1 = \\mathbf{b}_1\\)).\nDie Laufzeit liegt somit in \\(O(n^2)\\).\ndef forward_sub(lower, rhs):\n    n = lower.shape[0]\n    solution = np.zeros(n)\n    for i in range(n):\n        solution[i] = rhs[i]\n        for j in range(i):\n            solution[i] -= lower[i, j] * solution[j]\n            solution[i] = solution[i] / lower[i, i]\n    return solution\nforward_sub(np.array([\n    [1, 0, 0], \n    [2, 1, 0], \n    [3, 4, 1]]\n), np.array([1, 2, 3]))\n\narray([1., 0., 0.])",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Direkte L√∂sungsverfahren f√ºr lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#r√ºckw√§rts-substitution",
    "href": "02-lgs.html#r√ºckw√§rts-substitution",
    "title": "2¬† Direkte L√∂sungsverfahren f√ºr lineare Gleichungen",
    "section": "2.2 R√ºckw√§rts-Substitution",
    "text": "2.2 R√ºckw√§rts-Substitution\n\\[\nR = \\begin{pmatrix}\nr_{11} & r_{12} & r_{13} & \\dots  & r_{1n} \\\\\n    & r_{22} & r_{23} & \\dots  & r_{2n} \\\\\n    &        & r_{33} & \\dots  & r_{3n} \\\\\n    &        &        & \\ddots & \\vdots \\\\\n    &        &        &        & r_{nn}\n\\end{pmatrix}\n\\]\nDie R√ºckw√§rts-Substitution l√∂st \\(R \\cdot \\mathbf{x}=\\mathbf{y}\\), indem wir von der letzten Zeile aus das verfahren der Vorw√§rts-Substitution anwenden.\nDie Laufzeit liegt somit ebenfalls in \\(O(n^2)\\).\n\ndef backward_sub(upper, rhs):\n    n = upper.shape[0]\n    solution = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        tmp = rhs[i]\n        for j in range(i + 1, n):\n            tmp -= upper[i, j] * solution[j]\n            solution[i] = tmp / upper[i, i]\n    return solution\n\n\nbackward_sub(np.array([\n    [2, 2, 3], \n    [0, 1, 4], \n    [0, 0, 1]]\n), np.array([1, 0, 0]))\n\narray([0.5, 0. , 0. ])",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Direkte L√∂sungsverfahren f√ºr lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#lr-zerlegung",
    "href": "02-lgs.html#lr-zerlegung",
    "title": "2¬† Direkte L√∂sungsverfahren f√ºr lineare Gleichungen",
    "section": "2.3 LR-Zerlegung",
    "text": "2.3 LR-Zerlegung\n(en. LU-Decomposition)\n\n\n\n\n\n\n\nWarnung\n\n\n\nDie \\(1\\)-en auf der Diagonalen der \\(L\\)-Matrix bleiben beim Zeilentauschen unver√§ndert\n\n\nDie \\(LR\\)-Zerlegung l√§sst sich mittels des Gau√ü-Algorithmus bestimmen, indem wir \\(A\\) auf eine untere Dreiecksmatrix \\(R\\) gau√üen und uns die Operationen in \\(L\\) ‚Äúmerken‚Äù. Sie ist eindeutig und ben√∂tigt \\(O(n^3)\\) Operationen.\nDarauf l√§sst sich die L√∂sung mittels Vorw√§rtssubsitution von \\(L\\cdot\\mathbf{y}=\\mathbf{b}\\) (oder \\(L\\cdot\\mathbf{y}=P \\cdot \\mathbf{b}\\) mit \\(P\\) als Permutationsmatrix) und R√ºckw√§rtssubstitution von \\(R\\cdot\\mathbf{x}=\\mathbf{y}\\) aufbauen.\nDie Berechnung ist nicht stabil.\nHinreichende Bedingungen f√ºr die Exsistenz einer \\(LR\\)-Zerlegung f√ºr eine quadratische Matrix \\(A\\):\n\nstrikt diagonal-dominant, daher das Diagonalelement ist gr√∂√üer als die Summe aller anderen Elemente in der Zeile, bzw.\n\n\\[\n|A[n, n]|&gt;\\sum_{\\substack{k=1 \\\\ k \\neq n}}^N|A[n, k]| \\quad \\text { f√ºr } n=1, \\ldots, N\n\\]\n\npositiv definit, daher alle Eigenwerte \\(&gt; 0\\), bzw.\n\n\\[\nx^{\\top} A x&gt;0 \\quad \\text { f√ºr alle } x \\in \\mathbb{R}^N, x \\neq 0 .\n\\]\n\n\n\n\n\n\nHinweis\n\n\n\n\n\n\nHauptminorenkriterium f√ºr positiv definite Matrizen\n\n\n\n\nFalls diese Bedingungen nicht gegeben sind, k√∂nnen wir mittels Zeilenvertauschung (Permutationsmatrix \\(P\\)) eine LR-zerlegbare Matrix \\(PA\\) in \\(O(n^3)\\) erzeugen.\n\ndef lu_decomposition(matrix):\n    n = matrix.shape[0]\n    lower = np.zeros(shape=matrix.shape)\n    upper = np.zeros(shape=matrix.shape)\n    for j in range(n):\n        lower[j][j] = 1.0\n        for i in range(j + 1):\n            first_sum = sum(upper[k][j] * lower[i][k] for k in range(i))\n            upper[i][j] = matrix[i][j] - first_sum\n        for i in range(j, n):\n            second_sum = sum(upper[k][j] * lower[i][k] for k in range(j))\n            lower[i][j] = (matrix[i][j] - second_sum) / upper[j][j]\n    return lower, upper\n\ndef solve_with_lu(matrix, rhs):\n    lower, upper = lu_decomposition(matrix)\n    y = forward_sub(lower, rhs)\n    return backward_sub(upper, y)\n\n\nmatrix = np.array([[2.0, 1.0],\n[1.0, 4.0]])\nrhs = np.array([1.0, 2.0])\nsolution = solve_with_lu(matrix, rhs)\nprint(\"solution\", solution)\ntest = rhs - np.dot(matrix, solution)\nprint(\"test \",test)\n\nsolution [0.5 0. ]\ntest  [0.  1.5]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Direkte L√∂sungsverfahren f√ºr lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#sec-cholesky",
    "href": "02-lgs.html#sec-cholesky",
    "title": "2¬† Direkte L√∂sungsverfahren f√ºr lineare Gleichungen",
    "section": "2.4 Cholesky-Zerlegung",
    "text": "2.4 Cholesky-Zerlegung\n\n\n\n\n\n\n(2.7) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) symmetrisch und positiv definit. Dann existiert genau eine Cholesky-Zerlegung \\(A=L L^{\\top}\\) mit einer regul√§ren unteren Dreiecksmatrix \\(L\\).\n\n\nEs handelt sich somit um eine Spezialisierung der LR-Zerlegung f√ºr symmetrisch, positiv definite Matrizen.\n\\[\\begin{align}\nA &=\n\\begin{pmatrix}\n   a_{11} & a_{21} & a_{31}\\\\\n   a_{21} & a_{22} & a_{32}\\\\\n   a_{31} & a_{32} & a_{33}\\\\\n\\end{pmatrix}\\\\\n& =\n\\begin{pmatrix}\n   l_{11} & 0 & 0 \\\\\n   l_{21} & l_{22} & 0 \\\\\n   l_{31} & l_{32} & l_{33}\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n   l_{11} & l_{21} & l_{31} \\\\\n   0 & l_{22} & l_{32} \\\\\n   0 & 0 & l_{33}\n\\end{pmatrix} \\equiv L L^T \\\\\n&= \\begin{pmatrix}\n   l_{11}^2     & l_{21}l_{11} & l_{31}l_{11} \\\\\n   l_{21}l_{11} & l_{21}^2 + l_{22}^2& l_{31}l_{21}+l_{32}l_{22} \\\\\n   l_{31}l_{11} & l_{31}l_{21}+l_{32}l_{22} & l_{31}^2 + l_{32}^2+l_{33}^2\n\\end{pmatrix}\\end{align}\\]\n\n2.4.1 Berechnung\nDiagonalelemente:\n\\[l_{kk} = \\sqrt{a_{kk} - \\sum_{{\\color{teal}j}=1}^{k-1} l_{k{\\color{teal}j}}^2}\\]\nRest:\n\\[l_{{\\color{orange}i}k} = \\frac{1}{l_{kk}} \\left ( a_{ik} - \\sum_{{\\color{teal}j}=1}^{k-1} l_{{\\color{orange}i}{\\color{teal}j}}l_{k{\\color{teal}j}} \\right )\\]\n\ndef cholesky_decomposition(A):\n    n = matrix.shape[0]\n    lower = np.zeros(matrix.shape)\n    lower[0, 0] = np.sqrt(matrix[0, 0])\n    for n in range(1, n):\n        y = forward_sub(lower[:n, :n], matrix[n, :n]) # linalg.solve_triangular(lower[:n, :n], matrix[n, :n], lower=True)\n        lower[n, :n] = y\n        lower[n, n] = np.sqrt(matrix[n, n] - np.dot(y, y))\n    return lower\n\ndef solve_with_cholesky(matrix, rhs):\n    lower = cholesky_decomposition(matrix)\n    y = forward_sub(lower, rhs)\n    return backward_sub(lower.transpose(), y)\n\n\nmatrix = np.array([[2.0, 1.0],\n[1.0, 4.0]])\nrhs = np.array([1.0, 2.0])\nrhs = np.array([1.0, 2.0])\nsolution = solve_with_cholesky(matrix, rhs)\nprint(\"solution\",solution)\ntest = rhs - np.dot(matrix, solution)\nprint(\"test \",test)\n\nsolution [0.70710678 0.        ]\ntest  [-0.41421356  1.29289322]\n\n\n\nDie Cholesky-Zerlegung ist stabil: Es gilt \\(\\kappa_2(L)^2=\\kappa(A)\\)\nDie Berechnung der Cholesky-Zerlegung ben√∂tigt nur halbsoviele Operationen wie die Berechnung einer LR-Zerlegung.\nMatrizen mit einer geeigneten H√ºllenstruktur (viele Nullelemente wie bei der Bandmatrix) k√∂nnen effizienter gel√∂st werden (Bandmatrix in \\(O(NM^2)\\))\n\n\n\n\nSchematische Darstellung einer Bandmatrix",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Direkte L√∂sungsverfahren f√ºr lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "02-lgs.html#sec-qr",
    "href": "02-lgs.html#sec-qr",
    "title": "2¬† Direkte L√∂sungsverfahren f√ºr lineare Gleichungen",
    "section": "2.5 QR-Zerlegung",
    "text": "2.5 QR-Zerlegung\n\n\n\n\n\n\n(2.14) Satz (QR-Zerlegung)\n\n\n\nZu \\(A \\in \\mathbb{R}^{M \\times N}\\) existiert eine \\(Q R\\)-Zerlegung \\(A=Q R\\) in eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{M \\times M}\\) mit \\(Q^{\\top} Q=I_M\\) und eine obere Dreiecksmatrix \\(R \\in \\mathbb{R}^{M \\times N}\\) mit \\(R[m, n]=0\\) f√ºr \\(m&gt;n\\).\n\n\n\nDas LGS \\(Ax = b\\) kann durch die Berechnung \\(y = Q^\\top b\\) und darauf mit R√ºcksubstitution \\(Rx = y\\) gel√∂st werden.\nAsymptotischer Aufwand in \\(O(N^3)\\)\n\nRotationen und Drehungen sind orthogonale Matrizen \\(Q \\in \\mathbb{R}^{N \\times N}\\) mit\n\n\\(Q Q^{\\top}=I_N, \\ Q^{\\top} Q=I_N\\), so dass \\(Q^{-1}=Q^{\\top}\\),\n\\(|Q v|_2=|v|_2\\) und \\((Q v)^{\\top}(Q w)=v^{\\top} w\\) L√§ngen und Winkel erhaltend,\n\\(\\kappa_2(Q)=1\\).\n\n\n2.5.1 Householder Transformationen\nWir erhalten \\(N\\) orthogonale Vektoren f√ºr \\(A\\), indem wir den ersten Spaltenvektor \\(v_1\\) mittels einer Householder Transformation (Spiegelung) \\(Q_1\\) auf die \\(x\\)-Achse (\\(e_1\\)) abbilden und dies sukzessiv f√ºr die n√§chsten Spaltenvektoren \\(v_i\\) aus \\(Q_1 \\cdot \\dots \\cdot Q_{i-1} \\cdot A\\) wiederholen (dabei vernachl√§ssigen wir die ersten \\(i\\) Zeilen, da wir nur einen Untervektorraum in \\(\\mathbb{R}^{N-i}\\) betrachten).\n\n\n\nQR-Zerlegung mittels Householder Transformationen berechnen\n\n\n\n\n2.5.2 Givens-Rotation\nAlternativ k√∂nnen wir Givens-Rotationen verwenden, um die Matrix \\(A\\) in eine obere Dreiecksmatrix zu √ºberf√ºhren. Eine Givens-Rotation ist eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{N \\times N}\\), die die \\(m\\)-te und \\(n\\)-te Zeile von \\(A\\) rotiert, um die Elemente unterhalb der Hauptdiagonalen zu eliminieren.\n\n\n\nQR-Zerlegung mittels Givens-Rotation\n\n\nBesonders bei d√ºnnbesetzten Matrizen (sparse) ist die Givens-Rotation effizient.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Direkte L√∂sungsverfahren f√ºr lineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "03-fitting.html",
    "href": "03-fitting.html",
    "title": "3¬† Lineare Ausgleichsrechnung",
    "section": "",
    "text": "3.1 Normalengleichung\nAnstatt eine extakte L√∂sung zu finden, k√∂nnen wir auch eine N√§herungsl√∂sung suchen, die die Summe der Abweichungen minimiert. Dies wird als lineare Ausgleichsrechnung bezeichnet.\n\\[\nA^\\top Ax = A^\\top b\n\\]\nist √§quivalent zur Minimierung von \\(\\|Ax - b\\|_2\\)\nL√∂sungsverfahren:\nFalls \\(A^\\top A\\) invertierbar und gut konditioniert:\nBerechne \\(QR\\)-Zerlegung (Kapitel 2.5) von \\(A=QR\\)\n\\[R[1:N, 1:N]x = (Q^T b)[1:N]\\] l√∂sen\n-oder-\nBerechne \\(A^\\top A\\) mittels Cholesky-Zerlegung (Kapitel 2.4).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#normalengleichung",
    "href": "03-fitting.html#normalengleichung",
    "title": "3¬† Lineare Ausgleichsrechnung",
    "section": "",
    "text": "Wenn \\(A^\\top A\\) invertierbar ist, ist die L√∂sung eindeutig.\nWenn \\(\\operatorname{Kern} \\{v \\in \\mathbb{R}^N : Av=0\\} \\neq \\{0\\}\\), dann ist die L√∂sung nicht eindeutig. In diesem Fall ist das Problem nicht sachgem√§√ü gestellt!",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#singularit√§tszerlegung",
    "href": "03-fitting.html#singularit√§tszerlegung",
    "title": "3¬† Lineare Ausgleichsrechnung",
    "section": "3.2 Singularit√§tszerlegung",
    "text": "3.2 Singularit√§tszerlegung\n(en. Singular Value Decomposition, SVD)\n\nFalls \\(A\\) singul√§r (nicht invertierbar) oder schlecht konditioniert ist, kann die Singularit√§tszerlegung verwendet werden um die Normalengleichung zu l√∂sen.\n\n\n\n\n\n\n(3.4) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{K \\times N}\\) mit \\(R = \\text{rang} A\\). Dann existieren Singul√§rwerte \\(\\sigma_1, \\dots, \\sigma_R &gt; 0\\), Matrizen \\(V \\in \\mathbb{R}^{K \\times R}, U \\in \\mathbb{R}^{N \\times R}\\) mit \\(V^\\top V = I_R = U^\\top U\\) und eine Zerlegung\n\\[\nA = U \\Sigma V^\\top\n\\]\nmit \\(\\Sigma = \\text{diag}(\\sigma_1, \\dots, \\sigma_R)= \\text{diag}(\\sqrt{\\lambda_1}, \\dots, \\sqrt{\\lambda_R}) \\in \\mathbb{R}^{R \\times R}\\).\n\n\nWir f√ºhren eine Drehung mit \\(V^\\top\\) durch, skalieren mit den Singularit√§tswerten und passen die Dimension von \\(N\\) auf \\(K\\) mit \\(\\Sigma\\) und drehen dann in \\(\\mathbb{R}^K\\) mittels \\(U\\).\n\n\\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_R &gt; 0\\) sind die geordneten Eigenwerte von \\(A^\\top A\\) und \\(A A^\\top\\).\n\\(U\\) enth√§lt die Eigenvektoren von \\(A A^\\top\\) als Spaltenvektoren und \\(V\\) die Eigenvektoren von \\(A^\\top A\\) als Zeilenvektoren (transponiert).\nEs gilt \\(A = \\sum_{k=1}^R \\sigma_k v_k u_k^\\top\\).\nDie Spalten \\(v_1,...,v_R\\) von V bilden eine ONB von \\(\\operatorname{Bild} A\\).\nDie Spalten \\(u_1,...,u_R\\) von U lassen sich mit \\(u_{R+1},...,u_N\\) zu einer ONB von \\(\\mathbb{R}^N\\) erg√§nzen. Dann ist \\(u_{R+1},...,u_N\\) eine ONB von \\(\\operatorname{Kern} A\\).\n\\(A^+ := \\sum_{k=1}^R \\frac{1}{\\sigma_k} u_k v_k^T = U \\Sigma^{-1} V^T\\) hei√üt Pseudo-Inverse.\nIst \\(A\\) regul√§r, so gilt \\(A^{-1} = A^+\\). Die Normalengleichung und damit \\(\\|Ax-b\\|_2 = \\min!\\) wird durch \\(x = A^+b\\) gel√∂st.\n\\(\\|A\\|_2 = \\sigma_1\\) und \\(\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_R}\\). Berechnung aufwendig, erfordert Eigenwertberechnung von \\(S = A^T A\\). Stabile Berechnung mit \\(O(N^3)\\) Operationen!\nDie Berechnung von \\(A^+b\\) ist f√ºr \\(\\sigma_R \\ll 1\\) numerisch nicht stabil.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "03-fitting.html#tikhonov-regularisierung",
    "href": "03-fitting.html#tikhonov-regularisierung",
    "title": "3¬† Lineare Ausgleichsrechnung",
    "section": "3.3 Tikhonov-Regularisierung",
    "text": "3.3 Tikhonov-Regularisierung\nStabilisiert die Singularit√§tszerlegung, indem ein Regularisierungsterm hinzugef√ºgt wird.\n\n\n\n\n\n\n(3.5) Satz\n\n\n\nZu \\(A \\in \\mathbb{R}^{M \\times N}, b \\in \\mathbb{R}^M, \\alpha &gt; 0\\) existiert genau ein \\(x^\\alpha \\in \\mathbb{R}^N\\), das die Tikhonov-Regularisierung \\[\nF_\\alpha(x) := \\frac{1}{2}|Ax-b|_2^2 + \\frac{\\alpha}{2}|x|_2^2\n\\] minimiert. Es gilt \\[\nx^\\alpha = (A^\\top A + \\alpha I_N)^{-1} A^\\top b.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Lineare Ausgleichsrechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html",
    "href": "04-eigenwerte.html",
    "title": "4¬† Eigenwertberechnung",
    "section": "",
    "text": "4.1 Hessenberg-Matrix\nüí° Idee: Suche iterativ eine zu \\(A=A_0\\) √§hnliche (gleiche Eigenwerte) Matrix \\(A_k\\), die wir mittels der QR-Zerlegung in eine obere Dreiecksmatrix \\(R\\) √ºberf√ºhren k√∂nnen:\n\\[\nA_k \\to R = \\begin{pmatrix} \\lambda_1 & * & \\cdots & * \\\\ & \\lambda_2 & \\ddots & \\vdots \\\\ & & \\ddots & * \\\\ & & & \\lambda_N \\end{pmatrix},\nQ_k \\to I_N\n\\]\nSomit lassen sich die Eigenwerte von \\(A_k\\) (\\(k \\to \\infty\\)) auf der Diagonalen ablesen (bzw. approximieren).\n‚ö†Ô∏è Problem: Normale QR-Zerlegung mit \\(A_{k+1} = R_k Q_k\\) ben√∂tigt \\(O(N^3)\\) Operationen.\n\\[\nH = \\begin{pmatrix}\n* & * & * & * & * \\\\\n* & * & * & * & * \\\\\n0 & * & * & * & * \\\\\n0 & 0 & * & * & * \\\\\n0 & 0 & 0 & * & *\n\\end{pmatrix}\n\\]\nEine Matrix \\(H \\in \\mathbb{R}^{N \\times N}\\) hei√üt (obere) Hessenberg-Matrix, wenn \\(H[n+2:N, n] = 0_{N-n-1}\\) f√ºr \\(n = 1,...,N-2\\) gilt.\nA = np.array([\n    [8, 12/5, -9/5],\n    [12/5, 109/25, 12/25],\n    [-9/5, 12/25, 116/25]\n])\n\nprint(H := scipy.linalg.hessenberg(A))\n\nprint(np.linalg.eig(A).eigenvalues)\nprint(np.linalg.eig(H).eigenvalues)\n\n[[8 -3 0]\n [-3 4 0]\n [0 0 5]]\n[2244241/233640 559439/233640 5]\n[2244241/233640 559439/233640 5]",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html#hessenberg-matrix",
    "href": "04-eigenwerte.html#hessenberg-matrix",
    "title": "4¬† Eigenwertberechnung",
    "section": "",
    "text": "(4.3) Satz\n\n\n\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\). Dann existiert eine orthogonale Matrix \\(Q \\in \\mathbb{R}^{N \\times N}\\), so dass \\[\nH = QAQ^{\\top}\n\\] eine Hessenberg-Matrix ist.\nDie Berechnung von \\(Q\\) ben√∂tigt \\(O(N^3)\\) Operationen.\n\n\n\n\n\nBerechnung der Hessenberg-Matrix",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html#inverse-iteration-mit-shift",
    "href": "04-eigenwerte.html#inverse-iteration-mit-shift",
    "title": "4¬† Eigenwertberechnung",
    "section": "4.2 Inverse Iteration mit Shift",
    "text": "4.2 Inverse Iteration mit Shift\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) symmetrisch.\n\nW√§hle \\(v^0 \\in \\mathbb{R}^N\\) mit \\(|v^0| = 1\\).  Setze \\(k := 0\\) und w√§hle \\(\\varepsilon &gt; 0\\).\nBerechne \\[\n\\mu_k = r(A, v^k) = \\frac{v^{\\top} A v}{v^{\\top} v},\n\\].  Falls \\(|Av^k - \\mu_k v^k| &lt; \\varepsilon\\): STOP.\nBerechne \\[\\begin{align*}\nw^k &= (A - \\mu_k I_N)^{-1}v^k, \\\\\nv^{k+1} &= \\frac{1}{|w^k|}w^k.\n\\end{align*}\\]\nSetze \\(k := k + 1\\) und gehe zu 1..\n\n\\(r(A, w) \\approx \\lambda\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "04-eigenwerte.html#qr-iteration-mit-shift",
    "href": "04-eigenwerte.html#qr-iteration-mit-shift",
    "title": "4¬† Eigenwertberechnung",
    "section": "4.3 QR-Iteration mit Shift",
    "text": "4.3 QR-Iteration mit Shift\nüí° Idee: Transformiere \\(A\\) auf √§hnliche Hessenbergform \\(H\\) und f√ºhre die QR-Zerlegung mit Shift in \\(O(N^2)\\) durch.\nIm Folgenden sei \\(H \\in \\mathbb{R}^{N \\times N}\\) symmetrisch, tridiagonal und irreduzibel (\\(H\\) kann z.B. eine Hessenberg-Matrix sein).\n\nW√§hle \\(\\varepsilon \\geq 0\\), setze \\(H_0 = H\\), \\(k := 0\\) und \\(n=N\\).\nFalls \\(|H_k[n, n-1]| \\leq \\varepsilon\\)  setze \\(n := n - 1\\),  falls \\(n = 1\\): STOP\nW√§hle \\(\\mu_k = H[n, n]\\).\nBerechne eine QR-Zerlegung \\(H_k - \\mu_k I_N = Q_k R_k\\).\nSetze  \\(H_{k+1} = R_k Q_k + \\mu_k I_N\\).  Setze \\(k := k + 1\\), gehe zu 1.\n\nEs gilt \\[\\begin{alignat*}{2}\nH_{k+1} &= R_k Q_k &&+ \\mu_k I_N \\\\\n&= Q_k^\\top (A_k - \\mu_k I_N) Q_k &&+ \\mu_k I_N \\\\\n&= Q_k^\\top A_k Q_k\n\\end{alignat*}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Eigenwertberechnung</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html",
    "href": "05-iter-lgs.html",
    "title": "5¬† Iterationsverfahren f√ºr lineare Gleichungssysteme",
    "section": "",
    "text": "5.1 Allgemeine lineare Iteration\nüèÅ Ziel: Approximierung eines LGS \\(Ax = b\\) mittels einer iterativen Methode und einer Genauigkeit \\(\\varepsilon\\)\n‚ö†Ô∏è Problem: \\(A\\) ggf. schlecht konditioniert\nüí° Idee: L√∂se \\(BAx=Bb\\) mit \\(B\\) invertierbar und \\(BA\\) kleinere Kondition als \\(A\\)\nSei \\(B \\in \\mathbb{R}^{N \\times N}\\) ein Vorkonditionierer.\nSomit erhalten wir f√ºr \\(x\\) eine Fixpunktaufgabe:\n\\[\nx = (I - BA)x + Bb = x + B(b - Ax)\n\\]\nZur Bestimmung von \\(B\\) zerlegen wir\n\\[\nA = L + D + R\n\\]\nmit der strikt unteren Dreiecksmatrix \\(L=\\text{lower}(A)\\), der Diagonalmatrix \\(D=\\text{diag}(A)\\) und der strikt oberen Dreiecksmatrix \\(R=\\text{upper}(A)\\). Damit erhalten wir die folgenden Verfahren:\nAlgorithmus\nBemerkungen",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#allgemeine-lineare-iteration",
    "href": "05-iter-lgs.html#allgemeine-lineare-iteration",
    "title": "5¬† Iterationsverfahren f√ºr lineare Gleichungssysteme",
    "section": "",
    "text": "\\(B = D^{-1}\\) Jacobi,\n\\(B = (L+D)^{-1}\\) Gau√ü-Seidel,\n\\(B = R^{-1}L^{-1}\\) mit \\(A \\approx LR\\) unvollst√§ndige LR-Zerlegung,\n\\(B = \\sum_{j=1}^J E_j A_j^{-1} E_j^{\\top}\\) (‚ÄúSubspace-Correction‚Äù) mit:  Sei \\(\\{1, \\dots, N\\} = I_1 \\cup \\dots \\cup I_J\\) eine (√ºberlappende) Zerlegung mit \\(I_j = \\{n_{j,1}, \\dots, n_{j,N_j}\\}\\). Definiere \\(A_j[i,k] = A[n_{j,i}, n_{j,k}]\\), d.h. \\(A_j \\in \\mathbb{R}^{N_j \\times N_j}\\) und \\(E_j \\in \\mathbb{R}^{N_j \\times N}\\) mit \\(E_j[i, n_{j,i}] = 1\\) ansonsten 0.\nBPX, FETI, FETI-DP, √ºberlappendes Schwarz-Verfahren, Mehrgitter-Verfahren, ‚Ä¶.\n\n\n\nW√§hle \\(x^0 \\in \\mathbb{R}^N\\) und \\(\\varepsilon &gt; 0\\).  Setze \\(k := 0\\) und \\(r^0 := b - Ax^0\\).\nFalls \\(|r^k| &lt; \\varepsilon\\): STOP.\nBerechne:  \\[\\begin{align*}\nc^k  &= Br^k, \\\\\nx^{k+1} &= x^k + c^k, \\\\\nr^{k+1} &= r^k - Ac^k.\n\\end{align*}\\]  Setze \\(k := k + 1\\) und gehe zu 1..\n\n\n\nEs gilt  \\[\\begin{align*}\nr^{k+1} &= (b-Ax^k) - Ac^k \\\\\n&= b-A(x^k+c^k) \\\\\n&= b-Ax^{k+1}\n\\end{align*}\\]\nSei \\(Ax = b\\). Dann ist  \\(|r^k| = |b-Ax^k| = |Ax-Ax^k| \\leq \\|A\\|\\|x-x^k\\|\\).  Also folgt aus S1):  \\(|r^k| &lt; \\varepsilon\\), so dass \\(|x-x^k| = |A^{-1}r^k| &lt; \\|A^{-1}\\| \\|r^k\\| &lt; \\|A^{-1}\\| \\varepsilon\\).  Daher muss \\(\\varepsilon\\) immer problemabh√§ngig gew√§hlt werden!\nF√ºr den Fehler \\(e^k = x-x^k\\) gilt: \\[\\begin{align*}\ne^{k+1} &= x-x^{k+1} \\\\\n&= x-x^k - Br^k \\\\\n&= x-x^k - B(b-Ax^k) \\\\\n&= x-x^k - B(Ax-Ax^k) \\\\\n&= (I_N - BA)(x-x^k).\n\\end{align*}\\] Hierbei handelt es sich um lineare Konvergenz, falls \\(\\|I_N - BA\\| &lt; 1\\), da \\(|x-x^k| \\leq \\|I_N - BA\\|^k \\|x-x^0\\|\\).\n\n\n\n\n\n\n\n(5.1) Satz\n\n\n\nSeien \\(A, B \\in \\mathbb{R}^{N \\times N}\\) mit \\(\\rho(I - BA) &lt; 1\\). Dann ist \\(A\\) invertierbar und f√ºr alle \\(x^0 \\in \\mathbb{R}^N\\) konvergiert die Iteration \\[\nx^{k+1} = x^k + B(b - Ax^k) \\quad k = 0, 1, 2, \\dots\n\\] linear, so dass gilt \\[\n\\lim_{k \\to \\infty} x^k = A^{-1}b.\n\\]\nDenn: Dann existiert eine Norm mit \\(||I_N - BA|| &lt; 1\\) und \\[\\begin{align*}\nA^{-1} &=  (BA)^{-1}B \\\\\n&= (I_N - I_N + BA)^{-1}B \\\\\n&= \\sum_{k=0}^{\\infty} (I_N - BA)^k B \\quad \\text{(Neumann-Reihe)}.\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#cr-speicherformat",
    "href": "05-iter-lgs.html#cr-speicherformat",
    "title": "5¬† Iterationsverfahren f√ºr lineare Gleichungssysteme",
    "section": "5.2 CR-Speicherformat",
    "text": "5.2 CR-Speicherformat\n(en. Compressed Row Storage CRS)\nIm Folgenden schauen wir uns Verfahren f√ºr d√ºnn besetzte Matrizen an. Hierf√ºr definieren wir folgendes Speicherformat:\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\) d√ºnn besetzt. Speichere alle von null verschiedenen Eintr√§ge von \\(A\\) in einem Vektor \\(a \\in \\mathbb{R}^M\\). Speichere die zugeh√∂rigen Spaltenindizes in einem Vektor \\(s \\in \\mathbb{N}^M\\). Nun m√ºssen wir wissen, wo die n√§chsten Zeilen in \\(a\\) anfangen. Speichere daf√ºr f√ºr jede Zeile \\(i\\) in einem Vektor \\(d \\in \\mathbb{N}^{O}\\), sodass \\(d_i = \\text{Anfangsindex der Zeile } i \\text{ in } a\\) und dementsprechend \\(d_{i+1} = \\text{Beginn der n√§chsten Zeile } i+1\\) in \\(a\\).\n\n\n\n\n\n\nBeispiel\n\n\n\n\n\n\nCRS Speicherformat",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#krylovraumverfahren",
    "href": "05-iter-lgs.html#krylovraumverfahren",
    "title": "5¬† Iterationsverfahren f√ºr lineare Gleichungssysteme",
    "section": "5.3 Krylovraum‚ÄìVerfahren",
    "text": "5.3 Krylovraum‚ÄìVerfahren",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "05-iter-lgs.html#cg-verfahren",
    "href": "05-iter-lgs.html#cg-verfahren",
    "title": "5¬† Iterationsverfahren f√ºr lineare Gleichungssysteme",
    "section": "5.4 CG-Verfahren",
    "text": "5.4 CG-Verfahren\nIst unsere Matrix \\(A\\) symmetrisch und positiv definit, so k√∂nnen wir das CG-Verfahren verwenden. Dieses ist ein iteratives Verfahren, welches die L√∂sung des LGS \\(Ax = b\\) approximiert.\n\n\n\nCG Algorithmus",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr lineare Gleichungssysteme</span>"
    ]
  },
  {
    "objectID": "06-newton.html",
    "href": "06-newton.html",
    "title": "6¬† Iterationsverfahren f√ºr nichtlineare Gleichungen",
    "section": "",
    "text": "6.1 Newton-Verfahren",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr nichtlineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "06-newton.html#newton-verfahren",
    "href": "06-newton.html#newton-verfahren",
    "title": "6¬† Iterationsverfahren f√ºr nichtlineare Gleichungen",
    "section": "",
    "text": "W√§hle den Startwert \\(x^0 \\in D\\) und die Fehlertoleranz \\(\\varepsilon\\). Setze \\(k := 0\\).\nFalls \\(|F(x^k)| &lt; \\varepsilon\\): STOP.\nBerechne die Newton-Korrektur \\(d^k\\): L√∂se \\[\nF'(x^k) \\cdot d^k = -F(x^k)\n\\]\nSetze  \\[\nx^{k+1} = x^k + d^k\n\\]  und \\(k := k + 1\\). Gehe dann zu S1).\n\n\n\n\n\n\n\n(6.2) Satz\n\n\n\nSei \\(D \\subset \\mathbb{R}^N\\) offen, \\(F \\in C^1(D, \\mathbb{R}^N)\\) und \\(x^* \\in D\\) mit \\(F(x^*) = 0_N\\). Falls ein \\(B \\in \\mathbb{R}^{N \\times N}\\) mit \\[\n\\rho(I - BF'(x^*)) &lt; 1\n\\] existiert, dann ist die Fixpunktiteration \\[\nx^{k+1} = \\Phi(x^k)\n\\] mit \\[\n\\Phi(x) = x - BF(x)\n\\] lokal linear konvergent, d.¬†h. es existiert ein \\(\\delta &gt; 0\\), \\(C &gt; 0\\) und \\(\\theta \\in (0,1)\\) mit \\[\n|x^k - x^*| \\leq C\\theta^k |x^* - x^0|\n\\] f√ºr alle \\(x^0 \\in B(x^*, \\delta)\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Iterationsverfahren f√ºr nichtlineare Gleichungen</span>"
    ]
  },
  {
    "objectID": "07-pol-interpol.html",
    "href": "07-pol-interpol.html",
    "title": "7¬† Polynom-Interpolation",
    "section": "",
    "text": "7.1 Monomenbasis\nGegeben sei eine Menge von St√ºtzstellen \\(\\xi_0, \\ldots, \\xi_N\\) und die zugeh√∂rigen Funktionswerte \\(f_0, \\ldots, f_N\\).\nüèÅ Ziel: Finde eine Funktion \\(f(\\xi_n) = f_n\\) f√ºr alle \\(n = 0, \\ldots, N\\).\nIm Folgenden betrachten wir verschiedene Darstellungsformen der Polynomen-Basis.\n\\[\nP(t) = \\sum_{n=0}^N a_n \\cdot t^n\n\\]\nZum L√∂sen der Interpolationsaufgabe muss der Koeffizientenvektor \\(a = [a‚ÇÄ;a‚ÇÅ; \\ldots ;a_N] \\in \\mathbb{R}^{N+1}\\) bestimmt werden, welcher \\[a‚ÇÄ+a‚ÇÅŒæ_n + a‚ÇÇŒæ_n^2 + \\ldots + a_NŒæ_n^N = f_n\\] f√ºr \\(n = 0, \\ldots, N\\) gen√ºgt. Dies f√ºhrt auf ein lineares Gleichungssystem \\(Ta = f\\) mit \\(f = [f‚ÇÄ; \\ldots; f_N] \\in \\mathbb{R}^{N+1}\\) und der Vandermonde-Matrix \\[T = (Œæ_n^k)_{n,k=0,\\ldots,N} = \\begin{pmatrix} 1 & Œæ_0 & Œæ_0^2 & \\ldots & Œæ_0^N \\\\ 1 & Œæ_1 & Œæ_1^2 & \\ldots & Œæ_1^N \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & Œæ_N & Œæ_N^2 & \\ldots & Œæ_N^N \\end{pmatrix} \\in \\mathbb{R}^{(N+1) \\times (N+1)}.\\] Diese Matrix ist zwar regul√§r (da die \\(Œæ_n\\) paarweise verschieden sind), hat aber in der Regel eine sehr schlechte Kondition.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynom-Interpolation</span>"
    ]
  },
  {
    "objectID": "07-pol-interpol.html#lagrange-darstellung",
    "href": "07-pol-interpol.html#lagrange-darstellung",
    "title": "7¬† Polynom-Interpolation",
    "section": "7.2 Lagrange-Darstellung",
    "text": "7.2 Lagrange-Darstellung\nEine andere M√∂glichkeit ist die Darstellung √ºber die Lagrange Basispolynome \\(L_k \\in \\mathbb{P}_N\\) mit \\[L_n(t) = \\prod_{\\substack{k=0 \\\\ k \\neq n}}^N \\frac{t - Œæ_k}{Œæ_n - Œæ_k}.\\] Dadruch, dass \\(t-Œæ_k\\) f√ºr \\(k \\neq n\\) Nullstellen aufweist und \\(Œæ_n - Œæ_k\\) das Polynom bei \\(t = Œæ_n\\) auf \\(1\\) normiert, ergibt sich die Bedingung \\[L_n(Œæ_k) = \\begin{cases} 1, & k = n, \\\\ 0, & k \\neq n. \\end{cases}\\] Damit erhalten wir die Lagrange-Darstellung des Interpolationspolynoms \\[P(t) = \\sum_{n=0}^N f_nL_n(t)\\]\n\n‚ö†Ô∏è Probleme:\n\nZus√§tzliche St√ºtzstellen erfordern eine Neukonstruktion des gesamten Polynoms.\nBasis oszilliert stark",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynom-Interpolation</span>"
    ]
  },
  {
    "objectID": "07-pol-interpol.html#newton-darstellung",
    "href": "07-pol-interpol.html#newton-darstellung",
    "title": "7¬† Polynom-Interpolation",
    "section": "7.3 Newton-Darstellung",
    "text": "7.3 Newton-Darstellung\nWir betrachten St√ºtzstellen \\(\\xi_1, \\ldots, \\xi_N\\) und die zugeh√∂rigen Funktionswerte \\(f(\\xi_1), \\ldots, f(\\xi_N)\\). Nach Newton-Schema rechnen wir die Koeffizienten \\(c_n\\) und bilden das Produkt mit dem jeweiligen Newton-Polynom \\(\\omega_n(t)\\):\n\\[\nP_N(t) = \\sum_{n=0}^{N} c_n \\cdot \\omega_n(t) \\quad \\text{mit} \\quad \\omega_n(t) = \\prod_{k=0}^{n-1} (t - \\xi_k)\n\\]\nAlternativ l√§sst sich das Newton-Polynom \\(\\omega_n(t)\\) auch rekursiv definieren:\n\\[\n\\omega_0(t) = 1, \\quad \\omega_n(t) = \\omega_{n-1}(t) \\cdot (t - \\xi_{n-1}) \\quad \\text{f√ºr} \\quad n = 1, \\ldots, N.\n\\]\n\n\n\nBeispiel f√ºr Newton-Darstellung\n\n\n\n\n\n\n\n\n(7.3) Satz\n\n\n\nSei \\(f \\in C^{N+1}[a,b]\\) mit \\(a &lt; b\\) und \\(a \\le t \\le b\\). Zu \\(a \\le \\xi_0 \\le \\xi_1 \\le \\dots \\le \\xi_N \\le b\\) definiere \\[\nf_n = \\frac{1}{e_n!} \\left(\\frac{d}{dt}\\right)^{e_n} f(\\xi_n).\n\\]\nF√ºr den Interpolationsfehler gilt dann \\[\nf(t) - P_N(t) = \\frac{\\omega_{N+1}(t)}{(N+1)!} \\left(\\frac{d}{dt}\\right)^{N+1} f(\\tau_t)\n\\] mit \\(\\tau_t \\in I := [\\min\\{\\xi_0, t\\}, \\max\\{t, \\xi_N\\}]\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynom-Interpolation</span>"
    ]
  },
  {
    "objectID": "08-splines.html",
    "href": "08-splines.html",
    "title": "8¬† Splines",
    "section": "",
    "text": "üèÅ Ziel: Finde eine Interpolation f√ºr eine Menge von Punkten \\(y_0, \\dots y_M \\in \\mathbb{R}^2\\)\n‚ö†Ô∏è Problem: Polynom-Interpolation gibt nur ein einziges Polynom zur√ºck und ist durch das Runge-Ph√§nomen anf√§llig f√ºr gro√üe Fehler bei gro√üen Grad.\nüí° Idee: Kombiniere mehrere Polynome zu einem St√ºckweise definierten Polynom (Spline \\(S\\)).\nEigenschaften vom kubischen Splines:\n\n\\(S \\in \\mathbb{P}_3\\) ist ein Polynom \\(3\\)-ten Grades\nGlattheit: \\[\nS_n(\\xi_n)=S_{n+1}(\\xi_n) \\\\\nS'_n(\\xi_n)=S'_{n+1}(\\xi_n) \\\\\nS''_n(\\xi_n)=S''_{n+1}(\\xi_n) \\\\\n\\]\nRandbedingungen:\n\nnat√ºrlich: \\(S''(a) = S''(b) = 0\\)\nhermite: \\(S'(a) = f'_0, \\quad S'(b) = f'_M\\)\nperiodisch: \\(S(a) = S(b), \\quad S'(a) = S'(b)\\)",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Splines</span>"
    ]
  },
  {
    "objectID": "09-fft.html",
    "href": "09-fft.html",
    "title": "9¬† Trigonometrische Interpolation und FFT",
    "section": "",
    "text": "Nicht klausurrelevant ü•≥",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Trigonometrische Interpolation und FFT</span>"
    ]
  },
  {
    "objectID": "10-integration.html",
    "href": "10-integration.html",
    "title": "10¬† Numerische Integration",
    "section": "",
    "text": "10.1 Quadraturformeln\nüèÅ Ziel: Approximiere \\(\\int_a^b f(x) \\, dx\\) f√ºr eine Funktion \\(f\\).\nüí° Idee: Funktion zun√§chst interpolieren und dann dieses Interpolationspolynom integrieren\nSei \\(\\Xi = \\{\\xi_0, \\xi_1, \\ldots, \\xi_N\\} \\subset [a,b]\\) eine Menge von St√ºtzstellen und \\(w_\\xi\\) die zugeh√∂rigen Gewichte. Dann ist die Quadraturformel:\n\\[\nQ_\\Xi(f)=\\sum_{\\xi \\in \\Xi} w_\\xi f(\\xi)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Numerische Integration</span>"
    ]
  },
  {
    "objectID": "10-integration.html#quadraturformeln",
    "href": "10-integration.html#quadraturformeln",
    "title": "10¬† Numerische Integration",
    "section": "",
    "text": "10.1.1 Ordnung\n\\(Q_\\Xi\\) hat Ordnung \\(k\\), wenn f√ºr alle Polynome \\(P\\) mit \\(\\deg(P) \\leq k\\) gilt:\n\\[\n\\int_a^b P(x) \\, dx = Q_\\Xi(P) \\quad Q_\\Xi \\text{ ist exakt}\n\\]\n\n\n10.1.2 Newton-Cotes-Formeln\n(Quadratur zu √§quidistanten St√ºtzstellen)\n\\[\n\\begin{aligned}\nN = 1: \\quad & I_1(f) = \\frac{b-a}{2} \\left( f(a) + f(b) \\right) \\hspace{2cm} & \\textit{Trapezregel} \\\\[1.5em]\nN = 2: \\quad & I_2(f) = \\frac{b-a}{6} \\left( f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b) \\right) \\hspace{1.2cm} & \\textit{Simpsonregel} \\\\[1.5em]\nN = 3: \\quad & w_0 = w_3 = \\frac{b-a}{8},\\quad w_1 = w_2 = \\frac{3(b-a)}{8} \\hspace{2cm} & \\textit{Newton‚Äôsche } \\frac{3}{8}\\text{-Regel} \\\\[1.5em]\nN = 4: \\quad & w_0 = w_4 = \\frac{7(b-a)}{90},\\quad w_1 = w_3 = \\frac{32(b-a)}{90},\\quad w_2 = \\frac{12(b-a)}{90} \\hspace{0.5cm} & \\textit{Milne-Regel}\n\\end{aligned}\n\\]\n\n\n10.1.3 Bemerkungen\n\nEs gibt zu allen Zerlegungen \\(\\Xi \\subset [a,b]\\) mit \\(\\#\\Xi=N+1\\) eine Quadraturformel \\(Q_\\Xi\\), welche exakt ist f√ºr alle Polynome \\(P\\) mit \\(\\deg(P) \\leq N\\).\n\nLagrarge Polynom: \\[\n  w_\\xi = \\int_a^b L_\\xi(t) \\, dt \\quad \\text{mit} \\quad L_\\xi(t) = \\prod_{\\eta \\in \\Xi, \\ \\eta \\neq \\xi} \\frac{t - \\eta}{\\xi - \\eta}\n  \\]\n\nDie Quadraturformel hat maximale Ordnung \\(2N-1\\) (Gau√ü-Quadratur)\n‚ö†Ô∏è Probleme:\n\n√Ñquidistante St√ºtzstellen (Newton-Cotes) f√ºhren f√ºr \\(N&gt;5\\) teilweise zu negativen Gewichten\nGau√ü-Quadratur ben√∂tigt spezielle St√ºzstellen. Keine adaptive ‚ÄúWiederverwertung‚Äù m√∂glich.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Numerische Integration</span>"
    ]
  },
  {
    "objectID": "10-integration.html#summierte-quadraturformel",
    "href": "10-integration.html#summierte-quadraturformel",
    "title": "10¬† Numerische Integration",
    "section": "10.2 Summierte Quadraturformel",
    "text": "10.2 Summierte Quadraturformel\nüí° Idee: Zerlege Intervall \\([a,b]=\\omega_1 \\cup \\dots \\cup \\omega_N\\), \\(\\omega_n=[a+(n-1)h, \\ a + nh]\\) mit √§quidistanter Schrittweite \\(h=\\frac{b-a}N\\) und verwende die Quadraturformel in jedem Intervall \\(\\omega_N\\).\n\n\n\n\n\n\n\n\nAbbildung¬†10.1: Verschiedene Quadraturformeln\n\n\n\n\n\n\n10.2.1 Summierte Trapezregel\nF√ºr \\(N \\in \\mathbb{N}\\) seien \\(h := \\frac{b-a}{N}\\) und\n\\[\n\\Xi := \\{\\xi_n = a + n h : n = 0, \\ldots, N\\}.\n\\]\nDefiniere dann zu \\(f \\in C[a,b]\\) die Quadraturformel:\n\\[\nT_N(f) := \\sum_{n=1}^N \\frac{h}{2} \\left( f(\\xi_{n-1}) + f(\\xi_n) \\right)\n= \\frac{h}{2} f(a) + h \\sum_{n=1}^{N-1} f(\\xi_n) + \\frac{h}{2} f(b)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Numerische Integration</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Mathematische Grundlagen",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "appendix.html#mathematische-grundlagen",
    "href": "appendix.html#mathematische-grundlagen",
    "title": "Appendix",
    "section": "",
    "text": "Analysis\n\nReihen und Summen\nGeometrische Reihe\nF√ºr alle reellen \\(q \\neq 1\\) und f√ºr alle \\(n \\in \\mathbb{N}_0\\) ist:\n\\[\n\\sum_{k=0}^n q^k=\\frac{1-q^{n+1}}{1-q}\n\\]\nDer Grenzwert ist dementsprechend:\n\\[\n\\sum_{k=0}^{\\infty} q^k=\\frac{1}{1-q}\n\\]\n\n\nLogarithmen\n\n\n\n\nLineare Algebra\n\n\nOrthogonale Matrizen\nEine Matrix \\(Q\\) ist orthogonal, wenn:\n\\[\nQ^\\top Q = Q Q^\\top = I\n\\]\n\n\\(Q^\\top = Q^{-1}\\)\n\n\\(Q\\) ist daher auch invertierbar\n\n\\(det(Q) = \\pm 1\\)\n\n\n\nSymmetrische Matrizen\nEine Matrix \\(S\\) ist symmetrisch, wenn:\n\\[\nS^\\top = S\n\\]\n\nDie Eigenwerte von \\(S\\) sind reell\n\nDie Eigenvektoren von \\(S\\) sind orthogonal zueinander\n\n\\(S^\\top S = S S^\\top\\) (normal)\nSei \\(A \\in \\mathbb{R}^{N \\times N}\\), dann ist \\(B=A^\\top A\\) symmetrisch\n\\(S\\) ist nicht notwendigerweise invertierbar\n\n\\(2\\times2\\)-Matrix invertieren\n\\[\nA=\\left(\\begin{array}{ll}\na & b \\\\\nc & d\n\\end{array}\\right) \\quad \\text { then } \\quad A^{-1}=\\frac{1}{a d-b c}\\left(\\begin{array}{cc}\nd & -b \\\\\n-c & a\n\\end{array}\\right)\n\\]",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referenzen",
    "section": "",
    "text": "Bartels, S√∂ren. 2016. Numerik 3x9: Drei Themengebiete in\njeweils neun kurzen Kapiteln. 1. Aufl. 2016.\nSpringer-Lehrbuch. Berlin Heidelberg: Springer Spektrum. https://doi.org/10.1007/978-3-662-48203-2.\n\n\nWei√ü, Daniel. 2024. ‚ÄúNumerische Mathematik f√ºr\ndie Fachrichtungen Informatik und Ingenieurwesen.‚Äù\nKarlsruher Institut f√ºr Technologie.\n\n\nWieners, Christian. 2025. ‚ÄúEinf√ºhrung in die\nNumerische Mathematik.‚Äù Karlsruher Institut\nf√ºr Technologie.",
    "crumbs": [
      "Referenzen"
    ]
  }
]